{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 37.823872,
     "end_time": "2025-02-27T01:38:09.941462",
     "exception": false,
     "start_time": "2025-02-27T01:37:32.11759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext cudf.pandas\n",
    "\n",
    "import numpy as np, pandas as pd, gc\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "print(f\"XGBoost version\",xgb.__version__)\n",
    "\n",
    "VER=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005763,
     "end_time": "2025-02-27T01:38:09.953867",
     "exception": false,
     "start_time": "2025-02-27T01:38:09.948104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Data\n",
    "We load train, train extra, and test data. The combined train data has 4 million rows! This means we do not need to fear overfitting train. We can make hundreds/thousands of new features and every time our CV improves our LB will improve too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.850698,
     "end_time": "2025-02-27T01:38:10.810724",
     "exception": false,
     "start_time": "2025-02-27T01:38:09.960026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "print(\"Train shape\", train.shape )\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.259661,
     "end_time": "2025-02-27T01:38:12.077263",
     "exception": false,
     "start_time": "2025-02-27T01:38:10.817602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train2 = pd.read_csv(\"data/training_extra.csv\")\n",
    "print(\"Extra Train shape\", train2.shape )\n",
    "train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.118584,
     "end_time": "2025-02-27T01:38:12.206821",
     "exception": false,
     "start_time": "2025-02-27T01:38:12.088237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.concat([train,train2],axis=0,ignore_index=True)\n",
    "print(\"Combined Train shape\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.198488,
     "end_time": "2025-02-27T01:38:12.41211",
     "exception": false,
     "start_time": "2025-02-27T01:38:12.213622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\")\n",
    "print(\"Test shape\", test.shape )\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Engineer 8 new columns by combining each categorical column with Weight Capacity as was done in my starter notebook. Next we engineer 63 more columns listed below:\n",
    "* One column to indicate all NANs using a base-2 encoding\n",
    "* NANs per feature combined with Weight Capacity\n",
    "* Bin Weight Capacity by rounding it in different ways\n",
    "* Merge original dataset price\n",
    "* Extract digits from Weight Capacity\n",
    "* Combine digit features\n",
    "* Combine original categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.02097,
     "end_time": "2025-02-27T01:38:12.45397",
     "exception": false,
     "start_time": "2025-02-27T01:38:12.433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CATS = list(train.columns[1:-2])\n",
    "print(f\"There are {len(CATS)} categorical columns:\")\n",
    "print( CATS )\n",
    "print(f\"There are 1 numerical column:\")\n",
    "print( [\"Weight Capacity (kg)\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.178897,
     "end_time": "2025-02-27T01:38:15.639301",
     "exception": false,
     "start_time": "2025-02-27T01:38:12.460404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "COMBO = [\"NaNs\"]\n",
    "train[\"NaNs\"] = np.float32(0)\n",
    "test[\"NaNs\"] = np.float32(0)\n",
    "\n",
    "for i,c in enumerate(CATS):\n",
    "\n",
    "    # NEW FEATURE - ENCODE ALL NAN AS ONE BASE-2 FEATURE\n",
    "    train[\"NaNs\"] += train[c].isna()*2**i\n",
    "    test[\"NaNs\"] += test[c].isna()*2**i\n",
    "\n",
    "    # NEW FEATURE - COMBINE EACH COLUMN'S NAN WITH WEIGHT CAPACITY\n",
    "    n = f\"{c}_nan_wc\"\n",
    "    train[n] = train[c].isna()*100 + train[\"Weight Capacity (kg)\"]\n",
    "    test[n] = test[c].isna()*100 + test[\"Weight Capacity (kg)\"]\n",
    "    COMBO.append(n)\n",
    "    \n",
    "    combine = pd.concat([train[c],test[c]],axis=0)\n",
    "    combine,_ = pd.factorize(combine)\n",
    "    train[c] = combine[:len(train)].astype(\"float32\")\n",
    "    test[c] = combine[len(train):].astype(\"float32\")\n",
    "    n = f\"{c}_wc\"\n",
    "    train[n] = train[c]*100 + train[\"Weight Capacity (kg)\"]\n",
    "    test[n] = test[c]*100 + test[\"Weight Capacity (kg)\"]\n",
    "    COMBO.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.074026,
     "end_time": "2025-02-27T01:38:15.7212",
     "exception": false,
     "start_time": "2025-02-27T01:38:15.647174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NEW FEATURE - BIN WEIGHT CAPACITY USING ROUNDING\n",
    "for k in range(7,10):\n",
    "    n = f\"round{k}\"\n",
    "    train[n] = train[\"Weight Capacity (kg)\"].round(k)\n",
    "    test[n] = test[\"Weight Capacity (kg)\"].round(k)\n",
    "    COMBO.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.524302,
     "end_time": "2025-02-27T01:38:16.252588",
     "exception": false,
     "start_time": "2025-02-27T01:38:15.728286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NEW FEATURE - ORIGINAL DATASET PRICE\n",
    "NEW_COLS = []\n",
    "orig = pd.read_csv(\"data/Noisy_Student_Bag_Price_Prediction_Dataset.csv\")\n",
    "tmp = orig.groupby(\"Weight Capacity (kg)\").Price.mean()\n",
    "tmp.name = \"orig_price\"\n",
    "train = train.merge(tmp, on=\"Weight Capacity (kg)\", how=\"left\")\n",
    "test = test.merge(tmp, on=\"Weight Capacity (kg)\", how=\"left\")\n",
    "NEW_COLS.append(\"orig_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.311771,
     "end_time": "2025-02-27T01:38:16.571633",
     "exception": false,
     "start_time": "2025-02-27T01:38:16.259862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NEW FEATURE - ORIGINAL DATASET PRICE FROM ROUNDED WEIGHT CAPACITY \n",
    "for k in range(7,10):\n",
    "    n = f\"round{k}\"\n",
    "    orig[n] = orig[\"Weight Capacity (kg)\"].round(k)\n",
    "    tmp = orig.groupby(n).Price.mean()\n",
    "    tmp.name = f\"orig_price_r{k}\"\n",
    "    train = train.merge(tmp, on=n, how=\"left\")\n",
    "    test = test.merge(tmp, on=n, how=\"left\")\n",
    "    NEW_COLS.append(f\"orig_price_r{k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.351268,
     "end_time": "2025-02-27T01:38:16.93051",
     "exception": false,
     "start_time": "2025-02-27T01:38:16.579242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NEW FEATURE - DIGIT EXTRACTION FROM WEIGHT CAPACITY\n",
    "for k in range(1,10):\n",
    "    train[f'digit{k}'] = ((train['Weight Capacity (kg)'] * 10**k) % 10).fillna(-1).astype(\"int8\")\n",
    "    test[f'digit{k}'] = ((test['Weight Capacity (kg)'] * 10**k) % 10).fillna(-1).astype(\"int8\")\n",
    "DIGITS = [f\"digit{k}\" for k in range(1,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.070447,
     "end_time": "2025-02-27T01:38:17.008149",
     "exception": false,
     "start_time": "2025-02-27T01:38:16.937702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NEW FEATURE - COMBINATIONS OF DIGITS \n",
    "for i in range(4):\n",
    "    for j in range(i+1,5):\n",
    "        n = f\"digit_{i+1}_{j+1}\"\n",
    "        train[n] = ((train[f'digit{i+1}']+1)*11 + train[f'digit{j+1}']+1).astype(\"int8\")\n",
    "        test[n] = ((test[f'digit{i+1}']+1)*11 + test[f'digit{j+1}']+1).astype(\"int8\")\n",
    "        COMBO.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW FEATURE - COMBINATIONS OF CATS\n",
    "PAIRS = []\n",
    "for i,c1 in enumerate(CATS[:-1]):\n",
    "    for j,c2 in enumerate(CATS[i+1:]):\n",
    "        n = f\"{c1}_{c2}\"\n",
    "        m1 = train[c1].max()+1\n",
    "        m2 = train[c2].max()+1\n",
    "        train[n] = ((train[c1]+1 + (train[c2]+1)/(m2+1))*(m2+1)).astype(\"int8\")\n",
    "        test[n] = ((test[c1]+1 + (test[c2]+1)/(m2+1))*(m2+1)).astype(\"int8\")\n",
    "        COMBO.append(n)\n",
    "        PAIRS.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.145323,
     "end_time": "2025-02-27T01:38:17.160769",
     "exception": false,
     "start_time": "2025-02-27T01:38:17.015446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"New Train shape:\", train.shape )\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.015795,
     "end_time": "2025-02-27T01:38:17.184442",
     "exception": false,
     "start_time": "2025-02-27T01:38:17.168647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURES = CATS + [\"Weight Capacity (kg)\"] + COMBO + DIGITS + NEW_COLS\n",
    "print(f\"We now have {len(FEATURES)} columns:\")\n",
    "print( FEATURES )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.007909,
     "end_time": "2025-02-27T01:38:17.200232",
     "exception": false,
     "start_time": "2025-02-27T01:38:17.192323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## XGBoost with Feature Engineering GroupBy\n",
    "We train XGBoost with nested folds. We use the inner nested fold to create new features that aggregate the target `price`. We must do this to prevent target leakage. And we use the outer fold to create new features that do not aggregate the target `price`. In each k fold loop, we engineer new features using the advanced feature engineering technique `groupby(COL1)[COL2].agg(STAT)`. Since we are using [RAPIDS cuDF-Pandas][1], these groupby computations will run fast on GPU! And we will train our model quickly on GPU using XGBoost!\n",
    "\n",
    "**NEW FEATURES** Compared with our starter notebook, we add two new types of aggregations. Namely we groupby and compute quantiles. And we groupby and compute histogram bin counts! We also create two new division features. We divide aggregated count by nunique. And we divide aggregated std by count. (We also removed some features from my starter notebook which saves memory).\n",
    "\n",
    "**UPDATE** We reduce all engineered features to `float32` to reduce memory usage. (But we keep original column Weight Capacity (and it's combinations) as `float64` so as not to lose the original digits). \n",
    "\n",
    "[1]: https://rapids.ai/cudf-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.014772,
     "end_time": "2025-02-27T01:38:24.105978",
     "exception": false,
     "start_time": "2025-02-27T01:38:24.091206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STATISTICS TO AGGEGATE FOR OUR FEATURE GROUPS\n",
    "STATS = [\"mean\",\"std\",\"count\",\"nunique\",\"median\",\"min\",\"max\",\"skew\"]\n",
    "STATS2 = [\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.014334,
     "end_time": "2025-02-27T01:38:24.127947",
     "exception": false,
     "start_time": "2025-02-27T01:38:24.113613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUANTILES AND HISTOGRAM BINS TO AGGREGATE\n",
    "BINS=10\n",
    "QUANTILES = [5,10,40,45,55,60,90,95]\n",
    "def make_histogram(prices, bins=BINS, range_min=15, range_max=150):\n",
    "    hist, _ = np.histogram(prices, bins=bins, range=(range_min, range_max))\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5978.624362,
     "end_time": "2025-02-27T03:18:02.759964",
     "exception": false,
     "start_time": "2025-02-27T01:38:24.135602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "FOLDS = 7\n",
    "kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros((len(train)))\n",
    "pred = np.zeros((len(test)))\n",
    "\n",
    "# OUTER K FOLD\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "    print(f\"### OUTER Fold {i+1} ###\")\n",
    "\n",
    "    X_train = train.loc[train_index,FEATURES+['Price']].reset_index(drop=True).copy()\n",
    "    y_train = train.loc[train_index,'Price']\n",
    "\n",
    "    X_valid = train.loc[test_index,FEATURES].reset_index(drop=True).copy()\n",
    "    y_valid = train.loc[test_index,'Price']\n",
    "\n",
    "    X_test = test[FEATURES].reset_index(drop=True).copy()\n",
    "\n",
    "    # INNER K FOLD (TO PREVENT LEAKAGE WHEN USING PRICE)\n",
    "    kf2 = KFold(n_splits=FOLDS, shuffle=True, random_state=42)   \n",
    "    for j, (train_index2, test_index2) in enumerate(kf2.split(X_train)):\n",
    "        print(f\" ## INNER Fold {j+1} (outer fold {i+1}) ##\")\n",
    "\n",
    "        X_train2 = X_train.loc[train_index2,FEATURES+['Price']].copy()\n",
    "        X_valid2 = X_train.loc[test_index2,FEATURES].copy()\n",
    "\n",
    "        ### FEATURE SET 1 (uses price) ###\n",
    "        col = \"Weight Capacity (kg)\"\n",
    "        tmp = X_train2.groupby(col).Price.agg(STATS)\n",
    "        tmp.columns = [f\"TE1_wc_{s}\" for s in STATS]\n",
    "        X_valid2 = X_valid2.merge(tmp, on=col, how=\"left\")\n",
    "        for c in tmp.columns:\n",
    "            X_train.loc[test_index2,c] = X_valid2[c].values.astype(\"float32\")\n",
    "\n",
    "        ### FEATURE SET 2 (uses price) ###\n",
    "        for col in COMBO:\n",
    "            tmp = X_train2.groupby(col).Price.agg(STATS2)\n",
    "            tmp.columns = [f\"TE2_{col}_{s}\" for s in STATS2]\n",
    "            X_valid2 = X_valid2.merge(tmp, on=col, how=\"left\")\n",
    "            for c in tmp.columns:\n",
    "                X_train.loc[test_index2,c] = X_valid2[c].values.astype(\"float32\")\n",
    "\n",
    "        # AGGREGATE QUANTILES (uses price)\n",
    "        for k in QUANTILES:\n",
    "            result = X_train2.groupby('Weight Capacity (kg)').agg({'Price': lambda x: x.quantile(k/100)})\n",
    "            result.columns = [f\"quantile_{k}\"]\n",
    "            X_valid2 = X_valid2.merge(result, on=\"Weight Capacity (kg)\", how=\"left\")\n",
    "            X_train.loc[test_index2,f\"quantile_{k}\"] = X_valid2[f\"quantile_{k}\"].values.astype(\"float32\")\n",
    "\n",
    "        # AGGREGATE HISTOGRAMS (uses price)\n",
    "        tmp = X_train2.loc[~X_train2.orig_price.isna()].groupby(\"Weight Capacity (kg)\")[[\"Price\"]].agg(\"count\")\n",
    "        tmp.columns = ['ct']\n",
    "        X_train3 = X_train2.merge(tmp.loc[tmp['ct']>1],on=\"Weight Capacity (kg)\",how=\"left\")\n",
    "        X_train3 = X_train3.loc[~X_train3['ct'].isna()]\n",
    "        result = X_train3.groupby(\"Weight Capacity (kg)\")[\"Price\"].apply(make_histogram)\n",
    "        result = result.to_frame()['Price'].apply(pd.Series)\n",
    "        result.columns = [f\"histogram_{x}\" for x in range(BINS)]\n",
    "        X_valid2 = X_valid2.merge(result, on=\"Weight Capacity (kg)\", how=\"left\")\n",
    "        for c in [f\"histogram_{x}\" for x in range(BINS)]:\n",
    "            X_train.loc[test_index2,c] = X_valid2[c].values.astype(\"float32\")\n",
    "            \n",
    "        del result, X_train3, tmp\n",
    "        del X_train2, X_valid2\n",
    "        gc.collect()\n",
    "\n",
    "    ### FEATURE SET 1 (uses price) ###\n",
    "    col = \"Weight Capacity (kg)\"\n",
    "    tmp = X_train.groupby(col).Price.agg(STATS)\n",
    "    tmp.columns = [f\"TE1_wc_{s}\" for s in STATS]\n",
    "    tmp = tmp.astype(\"float32\")\n",
    "    X_valid = X_valid.merge(tmp, on=col, how=\"left\")\n",
    "    X_test = X_test.merge(tmp, on=col, how=\"left\")\n",
    "\n",
    "    ### FEATURE SET 2 (uses price) ###\n",
    "    for col in COMBO:\n",
    "        tmp = X_train.groupby(col).Price.agg(STATS2)\n",
    "        tmp.columns = [f\"TE2_{col}_{s}\" for s in STATS2]\n",
    "        tmp = tmp.astype(\"float32\")\n",
    "        X_valid = X_valid.merge(tmp, on=col, how=\"left\")\n",
    "        X_test = X_test.merge(tmp, on=col, how=\"left\")\n",
    "\n",
    "    # AGGREGATE QUANTILES (uses price)\n",
    "    for k in QUANTILES:\n",
    "        result = X_train.groupby('Weight Capacity (kg)').agg({'Price': lambda x: x.quantile(k/100)})\n",
    "        result.columns = [f\"quantile_{k}\"]\n",
    "        result = result.astype(\"float32\")\n",
    "        X_valid = X_valid.merge(result, on=\"Weight Capacity (kg)\", how=\"left\")\n",
    "        X_test = X_test.merge(result, on=\"Weight Capacity (kg)\", how=\"left\")\n",
    "\n",
    "    # AGGREGATE HISTOGRAMS (uses price)\n",
    "    tmp = X_train.loc[~X_train.orig_price.isna()].groupby(\"Weight Capacity (kg)\")[[\"Price\"]].agg(\"count\")\n",
    "    tmp.columns = ['ct']\n",
    "    X_train3 = X_train.merge(tmp.loc[tmp['ct']>1],on=\"Weight Capacity (kg)\",how=\"left\")\n",
    "    X_train3 = X_train3.loc[~X_train3['ct'].isna()]\n",
    "    result = X_train3.groupby(\"Weight Capacity (kg)\")[\"Price\"].apply(make_histogram)\n",
    "    result = result.to_frame()['Price'].apply(pd.Series)\n",
    "    result.columns = [f\"histogram_{x}\" for x in range(BINS)]\n",
    "    result = result.astype(\"float32\")\n",
    "    X_valid = X_valid.merge(result, on=\"Weight Capacity (kg)\", how=\"left\")\n",
    "    X_test = X_test.merge(result, on=\"Weight Capacity (kg)\", how=\"left\")\n",
    "    del result, X_train3, tmp\n",
    "\n",
    "    # COUNT PER NUNIQUE\n",
    "    X_train['TE1_wc_count_per_nunique'] = X_train['TE1_wc_count']/X_train['TE1_wc_nunique']\n",
    "    X_valid['TE1_wc_count_per_nunique'] = X_valid['TE1_wc_count']/X_valid['TE1_wc_nunique']\n",
    "    X_test['TE1_wc_count_per_nunique'] = X_test['TE1_wc_count']/X_test['TE1_wc_nunique']\n",
    "    \n",
    "    # STD PER COUNT\n",
    "    X_train['TE1_wc_std_per_count'] = X_train['TE1_wc_std']/X_train['TE1_wc_count']\n",
    "    X_valid['TE1_wc_std_per_count'] = X_valid['TE1_wc_std']/X_valid['TE1_wc_count']\n",
    "    X_test['TE1_wc_std_per_count'] = X_test['TE1_wc_std']/X_test['TE1_wc_count']\n",
    "\n",
    "    # CONVERT TO CATS SO XGBOOST RECOGNIZES THEM\n",
    "    X_train[CATS+DIGITS] = X_train[CATS+DIGITS].astype(\"category\")\n",
    "    X_valid[CATS+DIGITS] = X_valid[CATS+DIGITS].astype(\"category\")\n",
    "    X_test[CATS+DIGITS] = X_test[CATS+DIGITS].astype(\"category\")\n",
    "\n",
    "    # DROP PRICE THAT WAS USED FOR TARGET ENCODING\n",
    "    X_train = X_train.drop(['Price'],axis=1)\n",
    "\n",
    "    # DROP NON-TE CAT PAIRS\n",
    "    X_train = X_train.drop(PAIRS,axis=1)\n",
    "    X_valid = X_valid.drop(PAIRS,axis=1)\n",
    "    X_test = X_test.drop(PAIRS,axis=1)\n",
    "\n",
    "    # BUILD MODEL\n",
    "    model = XGBRegressor(\n",
    "        device=\"cuda\",\n",
    "        max_depth=6,  \n",
    "        colsample_bynode=0.3, \n",
    "        subsample=0.8,  \n",
    "        n_estimators=50_000,  \n",
    "        learning_rate=0.01,  \n",
    "        enable_categorical=True,\n",
    "        min_child_weight=10,\n",
    "        early_stopping_rounds=500,\n",
    "    )\n",
    "    \n",
    "    # TRAIN MODEL\n",
    "    COLS = X_train.columns\n",
    "    model.fit(\n",
    "        X_train[COLS], y_train,\n",
    "        eval_set=[(X_valid[COLS], y_valid)],  \n",
    "        verbose=500,\n",
    "    )\n",
    "\n",
    "    # PREDICT OOF AND TEST\n",
    "    oof[test_index] = model.predict(X_valid[COLS])\n",
    "    pred += model.predict(X_test[COLS])\n",
    "\n",
    "    # CLEAR MEMORY\n",
    "    del X_train, X_valid, X_test\n",
    "    del y_train, y_valid\n",
    "    if i != FOLDS-1: del model\n",
    "    gc.collect()\n",
    "\n",
    "pred /= FOLDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017467,
     "end_time": "2025-02-27T03:18:02.8012",
     "exception": false,
     "start_time": "2025-02-27T03:18:02.783733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Overall CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.662836,
     "end_time": "2025-02-27T03:18:04.481755",
     "exception": false,
     "start_time": "2025-02-27T03:18:02.818919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COMPUTE OVERALL CV SCORE\n",
    "true = train.Price.values\n",
    "s = np.sqrt(np.mean( (oof-true)**2.0 ) )\n",
    "print(f\"=> Overall CV Score = {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.051362,
     "end_time": "2025-02-27T03:18:04.551865",
     "exception": false,
     "start_time": "2025-02-27T03:18:04.500503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SAVE OOF TO DISK FOR ENSEMBLES\n",
    "np.save(f\"oof_v{VER}\",oof)\n",
    "print(\"Saved oof to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018055,
     "end_time": "2025-02-27T03:18:04.58848",
     "exception": false,
     "start_time": "2025-02-27T03:18:04.570425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.03039,
     "end_time": "2025-02-27T03:18:04.637229",
     "exception": false,
     "start_time": "2025-02-27T03:18:04.606839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"\\nIn total, we used {len(COLS)} features, Wow!\\n\")\n",
    "print( list(COLS) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017859,
     "end_time": "2025-02-27T03:18:04.673617",
     "exception": false,
     "start_time": "2025-02-27T03:18:04.655758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## XGB Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 4.902029,
     "end_time": "2025-02-27T03:18:09.593631",
     "exception": true,
     "start_time": "2025-02-27T03:18:04.691602",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "fig, ax = plt.subplots(figsize=(10, 20))\n",
    "xgb.plot_importance(model, max_num_features=100, importance_type='gain',ax=ax)\n",
    "plt.title(\"Top 100 Feature Importances (XGBoost)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"/kaggle/input/playground-series-s5e2/sample_submission.csv\")\n",
    "sub.Price = pred\n",
    "sub.to_csv(f\"submission_v{VER}.csv\",index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(sub.Price,bins=100)\n",
    "plt.title(\"Test Predictions\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10995111,
     "sourceId": 90274,
     "sourceType": "competition"
    },
    {
     "datasetId": 5560970,
     "sourceId": 9198133,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 223071113,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6047.948412,
   "end_time": "2025-02-27T03:18:15.230472",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-27T01:37:27.28206",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
