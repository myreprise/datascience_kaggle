{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84895,"databundleVersionId":10008389,"sourceType":"competition"},{"sourceId":9616093,"sourceType":"datasetVersion","datasetId":5868381},{"sourceId":207317996,"sourceType":"kernelVersion"},{"sourceId":208836199,"sourceType":"kernelVersion"},{"sourceId":209456830,"sourceType":"kernelVersion"},{"sourceId":210478717,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. INTRODUCTION\n<center>\n<img src=\"https://images.unsplash.com/photo-1507652955-f3dcef5a3be5?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\" width=1300 height=800 />\n</center>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### üß† Depression Risk Prediction Analysis\n\n### üìå Problem Statement\nThis dataset was collected through an anonymous survey conducted between January and June 2023, focusing on understanding depression risk factors among adults. The survey targeted both working professionals and students, collecting comprehensive information about their demographic details, academic/work life, lifestyle factors, mental health history, and current mental well-being status.\n\n### üìä Dataset Description\nThe dataset consists of three files:\n- **train.csv**: Training dataset with labeled depression risk\n- **test.csv**: Test dataset for predictions\n- **sample_submission.csv**: Template for submission format\n\n### üìã Column Descriptions\n\n1. **Name**: Identifier for participants (anonymized)\n2. **Gender**: Participant's gender identity\n3. **Age**: Participant's age\n4. **City**: Location of residence\n5. **Working Professional or Student**: Current occupation category\n6. **Profession**: Specific profession/field of work\n7. **Degree**: Educational qualification\n8. **CGPA**: Academic performance measure\n9. **Academic Pressure**: Level of pressure from academic responsibilities\n10. **Work Pressure**: Level of pressure from work responsibilities\n11. **Study Satisfaction**: Level of satisfaction with studies\n12. **Job Satisfaction**: Level of satisfaction with current job\n13. **Work/Study Hours**: Daily hours spent on work/study\n14. **Sleep Duration**: Average daily sleep hours\n15. **Dietary Habits**: Eating patterns and food preferences\n16. **Have you ever had suicidal thoughts ?**: History of suicidal ideation (Yes/No)\n17. **Financial Stress**: Level of stress related to financial situation\n18. **Family History of Mental Illness**: Presence of mental illness in family (Yes/No)\n\n**Target Variable:** **Depression**, Binary indicator (1 = Yes, 0 = No) of depression risk\n\n### üìà Evaluation Metric\n- The model performance is evaluated using **Accuracy Score**\n- Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)\n\n### üéØ Objectives\n1. Build a machine learning model to predict depression risk based on various life factors\n2. Identify key contributors to mental health challenges\n3. Analyze the relationship between different life aspects (academic, professional, personal) and depression risk\n4. Create a reliable predictive model for early depression risk assessment\n5. Understand the impact of lifestyle factors on mental health","metadata":{}},{"cell_type":"markdown","source":"# 2. IMPORTS","metadata":{}},{"cell_type":"code","source":"# Essential Python and System Libraries\nimport numpy as np\nimport pandas as pd\nimport string\nimport time\nimport sys\nimport os\nfrom copy import deepcopy\nfrom functools import partial\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom prettytable import PrettyTable\nfrom yellowbrick.classifier import ROCAUC, ClassificationReport\nfrom yellowbrick.features import PCA as PCAVisualizer\n%matplotlib inline\nsns.set(style='darkgrid', font_scale=1.2)\npd.set_option('display.max_columns', None)\n\n# Progress Tracking\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm as tqdm_notebook\ntqdm_notebook.get_lock().locks = []\n\n# Missing Value Analysis\nimport missingno as msno\n\n# Feature Engineering and Processing\nfrom sklearn.preprocessing import (\n    LabelEncoder, \n    StandardScaler, \n    MinMaxScaler,\n    PowerTransformer, \n    FunctionTransformer,\n    RobustScaler\n)\nfrom category_encoders import (\n    OneHotEncoder, \n    OrdinalEncoder, \n    CountEncoder, \n    CatBoostEncoder,\n    TargetEncoder\n)\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Unsupervised Learning for Feature Engineering\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.decomposition import PCA, TruncatedSVD, FastICA\nfrom sklearn.manifold import TSNE, Isomap\nfrom sklearn.feature_selection import (\n    SelectKBest,\n    chi2,\n    mutual_info_classif,\n    RFE\n)\n\n# Statistical Analysis\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy.optimize import minimize_scalar\n\n!pip install spacy\n!python -m spacy download en_core_web_md\n\nimport spacy\nimport numpy as np\nfrom difflib import SequenceMatcher\n\n# Model Selection and Evaluation\nfrom sklearn.model_selection import (\n    train_test_split,\n    cross_val_score,\n    StratifiedKFold,\n    KFold,\n    GridSearchCV,\n    RandomizedSearchCV\n)\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    roc_auc_score,\n    precision_recall_curve,\n    f1_score,\n    precision_score,\n    recall_score,\n    log_loss,\n    make_scorer\n)\n\n# Traditional Machine Learning Models\nfrom sklearn.ensemble import (\n    RandomForestClassifier,\n    GradientBoostingClassifier,\n    ExtraTreesClassifier,\n    AdaBoostClassifier,\n    BaggingClassifier,\n    StackingClassifier,\n    VotingClassifier, \n    HistGradientBoostingClassifier,\n\n)\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import (\n    LogisticRegression,\n    SGDClassifier,\n    RidgeClassifier,\n    LogisticRegressionCV\n)\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import (\n    LinearDiscriminantAnalysis,\n    QuadraticDiscriminantAnalysis\n)\n\n# Boosting Libraries\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, Pool\n\n# Deep Learning\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (\n    Dense, \n    Dropout, \n    BatchNormalization,\n    Input,\n    Concatenate\n)\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping,\n    ModelCheckpoint,\n    ReduceLROnPlateau\n)\nfrom tensorflow.keras.optimizers import Adam\n\n# Scikit-learn compatible wrapper for Keras (custom implementation)\nclass KerasClassifierWrapper(tf.keras.Model):\n    def fit(self, X, y, **kwargs):\n        return super().fit(X, y, **kwargs)\n    \n    def predict(self, X):\n        return np.argmax(super().predict(X), axis=1)\n    \n    def predict_proba(self, X):\n        return super().predict(X)\n\n# Hyperparameter Optimization\n!pip install optuna\n!pip install cmaes\nimport optuna\nfrom optuna.samplers import TPESampler, CmaEsSampler\n\n# Imbalanced Learning\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom imblearn.pipeline import Pipeline as ImbPipeline\n\n# For reproducibility\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\ntf.random.set_seed(RANDOM_STATE)","metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-11-18T13:56:14.617714Z","iopub.execute_input":"2024-11-18T13:56:14.620057Z","iopub.status.idle":"2024-11-18T13:57:17.18131Z","shell.execute_reply.started":"2024-11-18T13:56:14.61993Z","shell.execute_reply":"2024-11-18T13:57:17.179509Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. DATA","metadata":{}},{"cell_type":"code","source":"# Check for GPU availability\nglobal device\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    print(\"GPU is available\")\n    device = 'gpu'\n    # Enable memory growth for GPU\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\nelse:\n    print(\"GPU is not available\")\n    device = 'cpu'\n\n# Define paths\nKAGGLE_PATH = '/kaggle/input/depression-surveydataset-for-analysis'\nTRAIN_PATH = '/kaggle/input/playground-series-s4e11/train.csv'\nTEST_PATH = '/kaggle/input/playground-series-s4e11/test.csv'\nORIGINAL_PATH = f'{KAGGLE_PATH}/final_depression_dataset_1.csv'\n\n# Load datasets\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)\noriginal = pd.read_csv(ORIGINAL_PATH)\n\n# Create copies for backup\ntrain_copy = train.copy()\ntest_copy = test.copy()\noriginal_copy = original.copy()\n\n# Drop ID columns if present\nif 'id' in train.columns:\n    train.drop(columns=['id'], inplace=True)\nif 'id' in test.columns:\n    test.drop(columns=['id'], inplace=True)\n\n# Convert Depression from Yes/No to 1/0 in original dataset\ndepression_mapping = {'Yes': 1, 'No': 0}\noriginal['Depression'] = original['Depression'].map(depression_mapping)\n\n# Mark data sources\noriginal['original'] = 1\ntrain['original'] = 0\ntest['original'] = 0\n\n# Combine training data with original dataset\ntrain = pd.concat([train, original], axis=0).reset_index(drop=True)\n\n# Define target variable\ntargets = ['Depression']\ntarget = targets[0]\n\nprint(\"Dataset Shapes:\")\nprint(f\"Training set: {train.shape}\")\nprint(f\"Test set: {test.shape}\")\n\nprint(\"\\nValue distribution (%) in Depression column:\")\ndisplay(train[target].value_counts(normalize=True).round(3) * 100)\n\nprint(\"\\nSample of training data:\")\ndisplay(train.head())","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-11-18T14:17:15.621132Z","iopub.execute_input":"2024-11-18T14:17:15.621679Z","iopub.status.idle":"2024-11-18T14:17:16.602503Z","shell.execute_reply.started":"2024-11-18T14:17:15.621628Z","shell.execute_reply":"2024-11-18T14:17:16.601264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.1 MISSING VALUES","metadata":{}},{"cell_type":"code","source":"table = PrettyTable()\ntable.title = \"üîç Missing Value Analysis for Train and Test Sets\"\ntable.field_names = ['Column Name', 'Data Type', 'Train Missing %', 'Test Missing %']\n\n# Set alignment for better readability\ntable.align['Column Name'] = 'l'  # left align column names\ntable.align['Data Type'] = 'l'    # left align data types\ntable.align['Train Missing %'] = 'r'  # right align percentages\ntable.align['Test Missing %'] = 'r'   # right align percentages\n\nfor column in train.columns:\n    data_type = str(train[column].dtype)\n    # Calculate missing percentages with 4 decimal places\n    non_null_count_train = format(100 - (train[column].count()/train.shape[0]*100), '.4f')\n    if column != target:\n        non_null_count_test = format(100 - (test[column].count()/test.shape[0]*100), '.4f')\n    else:\n        non_null_count_test = \"NA\"\n    table.add_row([column, data_type, non_null_count_train, non_null_count_test])\n\nprint(table)","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-11-18T14:17:21.866257Z","iopub.execute_input":"2024-11-18T14:17:21.86674Z","iopub.status.idle":"2024-11-18T14:17:22.162696Z","shell.execute_reply.started":"2024-11-18T14:17:21.866688Z","shell.execute_reply":"2024-11-18T14:17:22.161264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<font size=\"3\">There are missing values, I will let the algorithms to handle the missing for now</font>","metadata":{}},{"cell_type":"markdown","source":"# 4. EDA","metadata":{}},{"cell_type":"markdown","source":"## 4.1 NUMERICAL FEATURES","metadata":{}},{"cell_type":"code","source":"# Set up the visualization style\nplt.style.use('seaborn')\nsns.set_palette(\"husl\")\n\n# Separate numerical and categorical columns\nnumerical_features = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# Remove target and original column from features\nif 'Depression' in numerical_features:\n    numerical_features.remove('Depression')\nif 'original' in numerical_features:\n    numerical_features.remove('original')\n\n# 1. Target Distribution across Numerical Features\nprint(\"üìä Target Distribution Across Numerical Features\")\nfig = plt.figure(figsize=(20, 5*((len(numerical_features)+1)//2)))\n\nfor idx, feature in enumerate(numerical_features, 1):\n    plt.subplot((len(numerical_features)+1)//2, 2, idx)\n    \n    # Create boxplot\n    sns.boxplot(x='Depression', y=feature, data=train)\n    plt.title(f'Depression Distribution by {feature}', pad=20)\n    plt.xlabel('Depression (0: No, 1: Yes)')\n    plt.ylabel(feature)\n    \nplt.tight_layout(pad=3.0)\nplt.show()\n\n# Additional Statistical Summary for Numerical Features\nprint(\"\\nüìà Statistical Summary of Numerical Features by Target\")\nfor feature in numerical_features:\n    print(f\"\\n{'-'*50}\")\n    print(f\"Feature: {feature}\")\n    print(train.groupby('Depression')[feature].describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:17:23.419621Z","iopub.execute_input":"2024-11-18T14:17:23.420814Z","iopub.status.idle":"2024-11-18T14:17:26.118425Z","shell.execute_reply.started":"2024-11-18T14:17:23.420757Z","shell.execute_reply":"2024-11-18T14:17:26.117204Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.2 CATEGORICAL FEATURES","metadata":{}},{"cell_type":"markdown","source":"### CLEANING","metadata":{}},{"cell_type":"code","source":"def process_sleep_duration(duration):\n    try:\n        # Handle NaN\n        if pd.isna(duration):\n            return np.nan, np.nan\n            \n        duration = str(duration).lower().replace('hours', '').strip()\n        \n        # Handle 'more than' or 'less than' cases\n        if 'more than' in duration:\n            return float(duration.replace('more than', '').strip()), 12  # assuming 12 as max\n        if 'less than' in duration or 'than' in duration:\n            return 0, float(duration.replace('less than', '').replace('than', '').strip())\n        \n        # Handle weekly hours - if number is too large, divide by 7\n        if '-' not in duration:\n            try:\n                hours = float(duration)\n                if hours > 24:  # if more than 24, assume it's weekly\n                    daily = hours/7\n                    return daily, daily\n                return hours, hours\n            except:\n                return np.nan, np.nan\n                \n        # Handle ranges\n        start, end = duration.split('-')\n        start = float(start)\n        end = float(end)\n        \n        # Fix reversed ranges (like 9-6 to 6-9)\n        if start > end:\n            start, end = end, start\n            \n        # If range is too large, divide by 7 (weekly to daily)\n        if end > 24:\n            start = start/7\n            end = end/7\n            \n        return start, end\n    except:\n        return np.nan, np.nan\n        \n# Function to transform categorical columns\ndef transform_categorical_columns(df):\n   # Convert 'Yes'/'No' to 1/0 and rename columns\n   df['family_history'] = (df['Family History of Mental Illness'].str.lower() == 'yes').astype(int)\n   df['suicidal'] = (df['Have you ever had suicidal thoughts ?'].str.lower() == 'yes').astype(int) \n   df['Working Professional or Student'] = (df['Working Professional or Student'].str.lower() == 'Working Professional').astype(int)\n   \n   # Convert Gender to binary (Female = 0, Male = 1)\n   df['Gender'] = (df['Gender'] == 'Male').astype(int)\n   \n   # Clean Dietary Habits - convert invalid categories to NaN\n   valid_dietary = ['Healthy', 'Unhealthy', 'Less Healthy', 'More Healthy', \n                   '5 Healthy', '5 Unhealthy', 'Moderate', 'No Healthy',\n                   'Less than Healthy']\n   df.loc[~df['Dietary Habits'].isin(valid_dietary), 'Dietary Habits'] = np.nan\n    \n   df[['Min_sleep', 'Max_sleep']] = df['Sleep Duration'].apply(process_sleep_duration).apply(pd.Series)\n    \n   # Drop the original columns\n   df = df.drop(['Family History of Mental Illness', 'Have you ever had suicidal thoughts ?', 'Sleep Duration'], axis=1)\n   return df\n\n# Apply transformations to both train and test\ntrain = transform_categorical_columns(train)\ntest = transform_categorical_columns(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:17:26.121174Z","iopub.execute_input":"2024-11-18T14:17:26.121689Z","iopub.status.idle":"2024-11-18T14:18:02.329061Z","shell.execute_reply.started":"2024-11-18T14:17:26.121633Z","shell.execute_reply":"2024-11-18T14:18:02.327674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_degree_and_related_columns(df):\n    \"\"\"\n    Clean and standardize degree-related columns in a DataFrame based on the exact set of degrees present.\n    \n    Parameters:\n        df (pandas.DataFrame): Input DataFrame containing 'Degree' column\n        \n    Returns:\n        pandas.DataFrame: Cleaned DataFrame with standardized degrees\n    \"\"\"\n    df = df.copy()\n    \n    # Exact degree standardization mapping based on the data\n    degree_standardization = {\n        # Bachelor's Degrees\n        \"btech\": \"B.Tech\",\n        \"b.tech\": \"B.Tech\",\n        \"b tech\": \"B.Tech\",\n        \"btech\": \"B.Tech\",\n        \"b b.tech\": \"B.Tech\",\n        \"mechanical engineer\": \"B.Tech\",\n        \n        \"bcom\": \"B.Com\",\n        \"b.com\": \"B.Com\",\n        \"b_com\": \"B.Com\",\n        \"b.m.com\": \"B.Com\",\n        \"degree\": \"B.Com\",  # Based on previous mapping\n        \n        \"bsc\": \"B.Sc\",\n        \"b.sc\": \"B.Sc\",\n        \n        \"barch\": \"B.Arch\",\n        \"b.arch\": \"B.Arch\",\n        \n        \"bca\": \"BCA\",\n        \"b.ca\": \"BCA\",\n        \"b bca\": \"BCA\",\n        \"bhca\": \"BCA\",\n        \n        \"ba\": \"BA\",\n        \n        \"bba\": \"BBA\",\n        \"b.ba\": \"BBA\",\n        \n        \"be\": \"BE\",\n        \n        \"bed\": \"B.Ed\",\n        \"b.ed\": \"B.Ed\",\n        \"a.ed\": \"B.Ed\",\n        \"e.ed\": \"B.Ed\",\n        \"i.ed\": \"B.Ed\",\n        \"g.ed\": \"B.Ed\",\n        \"j.ed\": \"B.Ed\",\n        \"k.ed\": \"B.Ed\",\n        \n        \"bpharm\": \"B.Pharm\",\n        \"b.pharm\": \"B.Pharm\",\n        \"b_pharm\": \"B.Pharm\",\n        \"b._pharm\": \"B.Pharm\",\n        \"s.pharm\": \"B.Pharm\",\n        \n        \"bhm\": \"BHM\",\n        \"b.h\": \"BHM\",\n        \"bh\": \"BHM\",\n        \n        # Master's Degrees\n        \"mtech\": \"M.Tech\",\n        \"m.tech\": \"M.Tech\",\n        \n        \"mcom\": \"M.Com\",\n        \"m.com\": \"M.Com\",\n        \n        \"msc\": \"MSc\",\n        \n        \"mca\": \"MCA\",\n        \"gca\": \"MCA\",\n        \"rca\": \"MCA\",\n        \"pca\": \"MCA\",\n        \n        \"ma\": \"MA\",\n        \"m.\": \"MA\",\n        \"m\": \"MA\",\n        \n        \"mba\": \"MBA\",\n        \n        \"me\": \"ME\",\n        \n        \"med\": \"M.Ed\",\n        \"m.ed\": \"M.Ed\",\n        \"m.m.ed\": \"M.Ed\",\n        \"m.b.ed\": \"M.Ed\",\n        \n        \"march\": \"M.Arch\",\n        \"m.arch\": \"M.Arch\",\n        \n        \"mpharm\": \"M.Pharm\",\n        \"m.pharm\": \"M.Pharm\",\n        \n        \"mhm\": \"MHM\",\n        \n        # Other Professional Degrees\n        \"llb\": \"LLB\",\n        \"llm\": \"LLM\",\n        \"phd\": \"PhD\",\n        \"mbbs\": \"MBBS\",\n        \"md\": \"MD\",\n        \n        # School Education\n        \"class12\": \"Class 12\",\n        \"class 12\": \"Class 12\",\n        \"12th\": \"Class 12\"\n    }\n    \n    # Names found in the degree column\n    names = {\n        'advait', 'vibha', 'gagan', 'eshita', 'navya', 'bian', 'kavya', \n        'vrinda', 'moham', 'magan', 'rupak', 'aadhya', 'banchal'\n    }\n    \n    # Cities found in the degree column\n    cities = {'pune', 'bhopal'}\n    \n    # Job titles found in the degree column\n    job_titles = {\n        'business analyst', 'travel consultant', 'financial analyst'\n    }\n    \n    def standardize_text(text):\n        \"\"\"Standardize text by converting to lowercase and removing punctuation/spaces\"\"\"\n        if pd.isna(text):\n            return ''\n        text = str(text).lower()\n        text = ''.join(c for c in text if c not in string.punctuation)\n        return text.replace(' ', '')\n    \n    def is_numeric(x):\n        \"\"\"Check if a value can be converted to float\"\"\"\n        try:\n            float(x)\n            return True\n        except (ValueError, TypeError):\n            return False\n    \n    def standardize_degree(degree, degree_std):\n        \"\"\"Standardize degree values based on mapping and patterns\"\"\"\n        if pd.isna(degree):\n            return degree\n            \n        # Return NaN for non-degree values\n        if (degree_std in job_titles or \n            degree_std in names or \n            degree_std in cities or \n            is_numeric(degree)):\n            return np.nan\n            \n        # Check standardization mapping\n        if degree_std in degree_standardization:\n            return degree_standardization[degree_std]\n        \n        # Handle special cases\n        if 'b.gender' in degree_std:\n            return np.nan\n        if 'b.study_hours' in degree_std:\n            return np.nan\n        if 'b.press' in degree_std:\n            return np.nan\n        if 'm.ui' in degree_std:\n            return np.nan\n            \n        return np.nan\n    \n    # Standardize degree text for comparison\n    df['Degree_std'] = df['Degree'].apply(standardize_text)\n    \n    # Handle job titles\n    job_mask = df['Degree_std'].apply(lambda x: any(job in x for job in job_titles))\n    df.loc[job_mask & df['Profession'].isna(), 'Profession'] = df.loc[job_mask, 'Degree']\n    \n    # Handle CGPA values - specific values seen in data: '5.65', '3.0', '8.95', '20'\n    cgpa_mask = df['Degree'].apply(lambda x: isinstance(x, (int, float)) or \n                                 (isinstance(x, str) and is_numeric(x)))\n    df.loc[cgpa_mask, 'CGPA'] = df.loc[cgpa_mask, 'Degree'].apply(\n        lambda x: float(x) if is_numeric(x) and float(x) <= 10 else np.nan)\n    \n    # Handle cities\n    city_mask = df['Degree_std'].isin(cities)\n    df.loc[city_mask & df['City'].isna(), 'City'] = df.loc[city_mask, 'Degree']\n    \n    # Standardize degrees\n    df['Degree'] = df.apply(lambda x: standardize_degree(x['Degree'], x['Degree_std']), axis=1)\n    \n    # Drop temporary standardization column\n    df = df.drop('Degree_std', axis=1)\n    \n    return df\n\ntrain = clean_degree_and_related_columns(train)\ntest = clean_degree_and_related_columns(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:18:02.331295Z","iopub.execute_input":"2024-11-18T14:18:02.331869Z","iopub.status.idle":"2024-11-18T14:18:09.377093Z","shell.execute_reply.started":"2024-11-18T14:18:02.331808Z","shell.execute_reply":"2024-11-18T14:18:09.375933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef clean_profession_column(df):\n    df = df.copy()\n    \n    # Standardize format: lowercase, remove punctuation and extra spaces\n    df['Profession_std'] = df['Profession'].fillna('').astype(str).str.lower()\n    df['Profession_std'] = df['Profession_std'].apply(lambda x: ''.join(c for c in x if c not in string.punctuation))\n    df['Profession_std'] = df['Profession_std'].str.replace(' ', '')\n    \n    # 1. Names to convert to NaN\n    names = {\n        'manvi', 'pranav', 'samar', 'simran', 'yogesh', 'yuvraj'\n    }\n    \n    # 2. Cities to be moved to City column if City is NaN\n    cities = {\n        'nagpur', 'patna', 'surat', 'visakhapatnam'\n    }\n    city_mask = df['Profession_std'].isin(cities)\n    df.loc[city_mask & df['City'].isna(), 'City'] = df.loc[city_mask, 'Profession']\n    \n    # 3. Degrees to be moved to Degree column if Degree is NaN\n    degrees = {\n        'bcom': 'B.Com',\n        'bed': 'B.Ed',\n        'bpharm': 'B.Pharm',\n        'bba': 'BBA',\n        'bca': 'BCA',\n        'be': 'BE',\n        'llm': 'LLM',\n        'med': 'M.Ed',\n        'mpharm': 'M.Pharm',\n        'mtech': 'M.Tech',\n        'mba': 'MBA',\n        'mbbs': 'MBBS',\n        'mca': 'MCA',\n        'md': 'MD',\n        'me': 'ME',\n        'phd': 'PhD'\n    }\n    \n    # Modified this part to avoid the mapping issue\n    for prof_std, degree in degrees.items():\n        mask = (df['Profession_std'] == prof_std) & df['Degree'].isna()\n        df.loc[mask, 'Degree'] = degree\n    \n    # 4. Invalid/nonsensical entries to convert to NaN\n    invalid_entries = {\n        '24th', '3m', 'moderate', 'no', 'name', 'profession', 'unhealthy', \n        'unveil', 'familyvirar', 'dev'\n    }\n    \n    # 5. Standardize similar professions\n    profession_standardization = {\n        'academic': 'Teacher',\n        'medicaldoctor': 'Doctor',\n        'financialanalyst': 'Financial Analyst',\n        'finanancialanalyst': 'Financial Analyst',\n        'businessanalyst': 'Business Analyst',\n        'softwareengineer': 'Software Engineer',\n        'civilengineer': 'Civil Engineer',\n        'mechanicalengineer': 'Mechanical Engineer',\n        'cityconsultant': 'City Consultant',\n        'educationalconsultant': 'Educational Consultant',\n        'familyconsultant': 'Family Consultant',\n        'travelconsultant': 'Travel Consultant',\n        'digitalmarketer': 'Digital Marketer',\n        'marketingmanager': 'Marketing Manager',\n        'hrmanager': 'HR Manager',\n        'citymanager': 'City Manager',\n        'graphicdesigner': 'Graphic Designer',\n        'uxuidesigner': 'UX/UI Designer',\n        'contentwriter': 'Content Writer',\n        'investmentbanker': 'Investment Banker',\n        'salesexecutive': 'Sales Executive',\n        'customersupport': 'Customer Support',\n        'researchanalyst': 'Research Analyst',\n        'workingprofessional': 'Working Professional'\n    }\n    \n    def standardize_profession(prof, prof_std):\n        if pd.isna(prof) or prof_std == '':\n            return np.nan\n            \n        # Return NaN for names, cities, and invalid entries\n        if (prof_std in names or prof_std in cities or \n            prof_std in invalid_entries or prof_std in degrees.keys()):\n            return np.nan\n            \n        # Return standardized profession if it exists\n        if prof_std in profession_standardization:\n            return profession_standardization[prof_std]\n            \n        # Return original profession if it's already standard\n        if prof in ['Accountant', 'Analyst', 'Architect', 'Chef', 'Chemist',\n                   'Consultant', 'Data Scientist', 'Doctor', 'Electrician',\n                   'Entrepreneur', 'Judge', 'Lawyer', 'Manager', 'Pharmacist',\n                   'Pilot', 'Plumber', 'Researcher', 'Student', 'Surgeon',\n                   'Teacher', 'Unemployed']:\n            return prof\n            \n        return np.nan\n    \n    # Apply standardization\n    df['Profession'] = df.apply(lambda x: standardize_profession(x['Profession'], x['Profession_std']), axis=1)\n    \n    # Drop temporary column\n    df = df.drop('Profession_std', axis=1)\n    \n    return df\n\n# Apply to both datasets\ntrain = clean_profession_column(train)\ntest = clean_profession_column(test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:18:09.380837Z","iopub.execute_input":"2024-11-18T14:18:09.381355Z","iopub.status.idle":"2024-11-18T14:18:16.283854Z","shell.execute_reply.started":"2024-11-18T14:18:09.381299Z","shell.execute_reply":"2024-11-18T14:18:16.282559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_city_column(df):\n    df = df.copy()\n    \n    # Standardize format: remove punctuation and extra spaces\n    df['City_std'] = df['City'].fillna('').astype(str).str.lower()\n    df['City_std'] = df['City_std'].apply(lambda x: ''.join(c for c in x if c not in string.punctuation))\n    df['City_std'] = df['City_std'].str.replace(' ', '')\n    \n    # Valid Indian cities and their standardized names\n    city_standardization = {\n        'agra': 'Agra',\n        'ahmedabad': 'Ahmedabad',\n        'bangalore': 'Bangalore',\n        'bhopal': 'Bhopal',\n        'ghopal': 'Bhopal',\n        'mhopal': 'Bhopal',\n        'chennai': 'Chennai',\n        'delhi': 'Delhi',\n        'moredelhi': 'Delhi',\n        'lessdelhi': 'Delhi',\n        'faridabad': 'Faridabad',\n        'ghaziabad': 'Ghaziabad',\n        'khaziabad': 'Ghaziabad',\n        'gurgaon': 'Gurgaon',\n        'hyderabad': 'Hyderabad',\n        'indore': 'Indore',\n        'jaipur': 'Jaipur',\n        'kalyan': 'Kalyan',\n        'malyan': 'Kalyan',\n        'nalyan': 'Kalyan',\n        'kanpur': 'Kanpur',\n        'iskanpur': 'Kanpur',\n        'kolkata': 'Kolkata',\n        'golkata': 'Kolkata',\n        'molkata': 'Kolkata',\n        'rolkata': 'Kolkata',\n        'tolkata': 'Kolkata',\n        'lucknow': 'Lucknow',\n        'ludhiana': 'Ludhiana',\n        'meerut': 'Meerut',\n        'morena': 'Morena',\n        'mumbai': 'Mumbai',\n        'nagpur': 'Nagpur',\n        'nashik': 'Nashik',\n        'patna': 'Patna',\n        'pune': 'Pune',\n        'rajkot': 'Rajkot',\n        'vaikot': 'Rajkot',\n        'srinagar': 'Srinagar',\n        'surat': 'Surat',\n        'thane': 'Thane',\n        'thani': 'Thane',\n        'vadodara': 'Vadodara',\n        'varanasi': 'Varanasi',\n        'vasaivirar': 'Vasai-Virar',\n        'sanvasaivirar': 'Vasai-Virar',\n        'unirar': 'Vasai-Virar',\n        'visakhapatnam': 'Visakhapatnam'\n    }\n    \n    # Names to convert to NaN\n    names = {\n        'aaradhya', 'abhinav', 'aditi', 'aditya', 'aishwarya', 'anvi', \n        'armaan', 'atharv', 'avni', 'ayansh', 'ayush', 'bhavna', 'chhavi', \n        'dhruv', 'gaurav', 'harsh', 'harsha', 'hrithik', 'ira', 'ivaan', \n        'jhanvi', 'kagan', 'kashish', 'keshav', 'khushi', 'krishna', 'leela', \n        'mahi', 'malyansh', 'mihir', 'mira', 'nalini', 'nandini', 'parth', \n        'pooja', 'pratham', 'pratyush', 'raghavendra', 'rashi', 'reyansh', \n        'saanvi', 'sara', 'saurav', 'shrey', 'siddhesh', 'tushar', 'vaanya', \n        'vaishnavi', 'vidhi', 'vidya', 'vikram'\n    }\n    \n    # Invalid entries to convert to NaN\n    invalid_entries = {\n        '3.0', 'city', 'no', 'no12', 'chemist', 'lawyer', 'researcher',\n        'mca', 'mcom', 'mtech', 'me', 'msc', 'lessthan5hours', \n        'lessthan5kalyan', 'galesabad', 'ishanabad', 'ishkarsh', 'ithal', \n        'itheg', 'kashk', 'kibara', 'krinda', 'moreadhyay', 'plata', 'unaly'\n    }\n    \n    def standardize_city(city, city_std):\n        if pd.isna(city) or city_std == '':\n            return np.nan\n            \n        # Return NaN for names and invalid entries\n        if city_std in names or city_std in invalid_entries:\n            return np.nan\n            \n        # Return standardized city if it exists\n        if city_std in city_standardization:\n            return city_standardization[city_std]\n            \n        return np.nan\n    \n    # Apply standardization\n    df['City'] = df.apply(lambda x: standardize_city(x['City'], x['City_std']), axis=1)\n    \n    # Drop temporary column\n    df = df.drop('City_std', axis=1)\n    \n    return df\n\n# Apply to both datasets\ntrain = clean_city_column(train)\ntest = clean_city_column(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:18:16.285575Z","iopub.execute_input":"2024-11-18T14:18:16.286103Z","iopub.status.idle":"2024-11-18T14:18:21.897036Z","shell.execute_reply.started":"2024-11-18T14:18:16.286044Z","shell.execute_reply":"2024-11-18T14:18:21.895836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_features = train.select_dtypes(include=['object']).columns.tolist()\n\ntrain[categorical_features].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:18:21.898397Z","iopub.execute_input":"2024-11-18T14:18:21.898795Z","iopub.status.idle":"2024-11-18T14:18:21.986612Z","shell.execute_reply.started":"2024-11-18T14:18:21.898745Z","shell.execute_reply":"2024-11-18T14:18:21.985526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train[categorical_features].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:18:21.988136Z","iopub.execute_input":"2024-11-18T14:18:21.988587Z","iopub.status.idle":"2024-11-18T14:18:22.011442Z","shell.execute_reply.started":"2024-11-18T14:18:21.988534Z","shell.execute_reply":"2024-11-18T14:18:22.010261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.style.use('seaborn')\nsns.set_palette(\"husl\")\n\n# Plot for each categorical feature\nfor col in categorical_features:\n    if col!=\"Name\":\n        # Create contingency table with normalized values (percentages)\n        contingency_table = pd.crosstab(train[col], train['Depression'], normalize='index')\n        \n        # Set style\n        sns.set(style=\"whitegrid\")\n        \n        # Create stacked bar plot\n        contingency_table.plot(kind=\"bar\", \n                        stacked=True, \n                        figsize=(20, 4))\n        \n        # Customize plot\n        plt.title(f\"Percentage Distribution of Depression across {col}\")\n        plt.xlabel(col)\n        plt.ylabel(\"Percentage\")\n        plt.legend(title=\"Depression\")\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-11-18T14:18:22.013036Z","iopub.execute_input":"2024-11-18T14:18:22.013439Z","iopub.status.idle":"2024-11-18T14:18:26.13191Z","shell.execute_reply.started":"2024-11-18T14:18:22.013398Z","shell.execute_reply":"2024-11-18T14:18:26.130666Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. FEATURE ENGINEERING","metadata":{}},{"cell_type":"markdown","source":"## 5.1 TF-IDF","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport pandas as pd\nimport numpy as np\n\ndef process_categorical_features(categorical_features, train, test, n, p):\n    \"\"\"\n    Process categorical features using TF-IDF and SVD without dropping original features.\n    \n    Parameters:\n        categorical_features (list): List of categorical column names to process\n        train (pd.DataFrame): Training dataset\n        test (pd.DataFrame): Test dataset\n        n (int): Maximum number of features for TF-IDF\n        p (int): Number of components for SVD\n        \n    Returns:\n        tuple: Processed (train, test) datasets with original features preserved\n    \"\"\"\n    # Create copies to avoid modifying original dataframes\n    train = train.copy()\n    test = test.copy()\n    \n    def tf_idf(train_df, test_df, column, n_features, n_components):\n        \"\"\"\n        Apply TF-IDF and SVD transformation to a categorical column.\n        \n        Parameters:\n            train_df (pd.DataFrame): Training data\n            test_df (pd.DataFrame): Test data\n            column (str): Column name to process\n            n_features (int): Maximum number of features for TF-IDF\n            n_components (int): Number of components for SVD\n            \n        Returns:\n            tuple: Processed (train_df, test_df)\n        \"\"\"\n        # Handle missing values\n        train_df[column] = train_df[column].fillna('')\n        test_df[column] = test_df[column].fillna('')\n        \n        # Convert to string\n        train_df[column] = train_df[column].astype(str)\n        test_df[column] = test_df[column].astype(str)\n        \n        try:\n            # TF-IDF transformation\n            vectorizer = TfidfVectorizer(max_features=n_features)\n            vectors_train = vectorizer.fit_transform(train_df[column])\n            vectors_test = vectorizer.transform(test_df[column])\n            \n            # SVD transformation\n            svd = TruncatedSVD(n_components=n_components)\n            x_pca_train = svd.fit_transform(vectors_train)\n            x_pca_test = svd.transform(vectors_test)\n            \n            # Create feature names\n            feature_names = [f\"{column}_tfidf_{i}\" for i in range(n_components)]\n            \n            # Convert to DataFrame\n            tfidf_df_train = pd.DataFrame(x_pca_train, columns=feature_names)\n            tfidf_df_test = pd.DataFrame(x_pca_test, columns=feature_names)\n            \n            # Concatenate with original dataframes\n            train_df = pd.concat([train_df, tfidf_df_train], axis=1)\n            test_df = pd.concat([test_df, tfidf_df_test], axis=1)\n            \n            print(f\"Successfully processed {column}\")\n            return train_df, test_df\n            \n        except Exception as e:\n            print(f\"Error processing {column}: {str(e)}\")\n            return train_df, test_df\n    \n    # Process each categorical feature\n    for column in categorical_features:\n        if column in train.columns:  # Only process if column exists\n            train, test = tf_idf(\n                train, \n                test, \n                column,\n                n,\n                p\n            )\n        else:\n            print(f\"Warning: Column {column} not found in the dataset\")\n    \n    return train, test\n\nn = 1000  # max features for TF-IDF\np = 5     # number of components for SVD\ntrain, test= process_categorical_features(categorical_features, train, test, n, p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:18:46.024136Z","iopub.execute_input":"2024-11-18T14:18:46.024568Z","iopub.status.idle":"2024-11-18T14:18:55.143118Z","shell.execute_reply.started":"2024-11-18T14:18:46.024529Z","shell.execute_reply":"2024-11-18T14:18:55.141745Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.2  CATEGORICAL ENCODING","metadata":{}},{"cell_type":"code","source":"# Global variables\noverall_best_score = 0\noverall_best_col = None\n\ndef OHE(train_df, test_df, cols, target):\n   '''\n   One hot encoding function that handles train and test together\n   '''\n   combined = pd.concat([train_df, test_df], axis=0)\n   for col in cols:\n       one_hot = pd.get_dummies(combined[col]).astype(int)\n       counts = combined[col].value_counts()\n       min_count_category = counts.idxmin()\n       one_hot = one_hot.drop(min_count_category, axis=1)\n       one_hot.columns = [str(f)+col+\"_OHE\" for f in one_hot.columns]\n       combined = pd.concat([combined, one_hot], axis=\"columns\")\n       combined = combined.loc[:, ~combined.columns.duplicated()]\n   \n   train_ohe = combined[:len(train_df)]\n   test_ohe = combined[len(train_df):]\n   test_ohe.reset_index(inplace=True, drop=True)\n   test_ohe.drop(columns=[target], inplace=True)\n   return train_ohe, test_ohe\n\ndef high_freq_ohe(train, test, extra_cols, target, n_limit=50):\n   '''\n   One hot encoding for high cardinality features\n   '''\n   train_copy = train.copy()\n   test_copy = test.copy()\n   \n   for col in extra_cols:\n       dict1 = train_copy[col].value_counts().to_dict()\n       ordered = dict(sorted(dict1.items(), key=lambda x: x[1], reverse=True))\n       rare_keys = list([*ordered.keys()][n_limit:])\n       rare_key_map = dict(zip(rare_keys, np.full(len(rare_keys), 9999)))\n       \n       train_copy[col] = train_copy[col].replace(rare_key_map)\n       test_copy[col] = test_copy[col].replace(rare_key_map)\n       \n   train_copy, test_copy = OHE(train_copy, test_copy, extra_cols, target)\n   drop_cols = [f for f in train_copy.columns if \"9999\" in f or train_copy[f].nunique()==1]\n   train_copy = train_copy.drop(columns=drop_cols)\n   test_copy = test_copy.drop(columns=drop_cols)\n   \n   return train_copy, test_copy\n\ndef handle_categorical_features(train, test, cat_cols):\n    '''\n    Initial processing of categorical features with proper handling of text categories\n    '''\n    cat_cols_updated = []\n    train_processed = train.copy()\n    test_processed = test.copy()\n    \n    for col in cat_cols:\n        print(f\"Processing {col}...\")\n        # Find uncommon categories\n        train_categories = set(train[col].unique())\n        test_categories = set(test[col].unique())\n        uncommon = list(train_categories.union(test_categories) - train_categories.intersection(test_categories))\n        \n        if train[col].dtype != \"O\":\n            # Handle numeric categorical\n            train_processed[f\"{col}_cat\"] = train[col]\n            test_processed[f\"{col}_cat\"] = test[col]\n            cat_cols_updated.append(f\"{col}_cat\")\n            \n            if uncommon:\n                train_processed[f\"{col}_cat\"] = train_processed[f\"{col}_cat\"].apply(\n                    lambda x: np.nan if x in uncommon else x)\n                test_processed[f\"{col}_cat\"] = test_processed[f\"{col}_cat\"].apply(\n                    lambda x: np.nan if x in uncommon else x)\n                \n        else:\n            # Handle string categorical\n            cat_cols_updated.append(col)\n            \n            # Create category maps using only training data\n            categories = train[col].unique()\n            cat_map = {cat: f\"{cat}_{col}\" for cat in categories}\n            \n            # Apply mapping to both train and test\n            train_processed[col] = train[col].map(cat_map).fillna('Other_' + col)\n            test_processed[col] = test[col].map(cat_map).fillna('Other_' + col)\n            \n            # Count encoding (will be used later)\n            train_processed[f\"{col}_count\"] = train[col].map(train[col].value_counts())\n            test_processed[f\"{col}_count\"] = test[col].map(train[col].value_counts().to_dict()).fillna(0)\n    \n    return train_processed, test_processed, cat_cols_updated\n\ndef cat_encoding(train, test, cat_cols_updated, target):\n    '''\n    Main encoding function with feature selection using Accuracy Score\n    '''\n    global overall_best_score\n    global overall_best_col\n    \n    table = PrettyTable()\n    table.field_names = ['Feature', 'Encoded Features', 'Accuracy Score']\n    \n    train_copy = train.copy()\n    test_copy = test.copy()\n    \n    for feature in cat_cols_updated:\n        print(f\"Encoding {feature}...\")\n        \n        # For numeric features\n        if train[feature].dtype != 'O':\n            # Count encoding\n            dic = train[feature].value_counts().to_dict()\n            train_copy[feature + \"_count\"] = train[feature].map(dic)\n            test_copy[feature + \"_count\"] = test[feature].map(dic)\n            \n            # Rank encoding\n            dic2 = train[feature].value_counts().to_dict()\n            list1 = np.arange(len(dic2.values()))\n            dic3 = dict(zip(list(dic2.keys()), list1))\n            \n            train_copy[feature+\"_count_label\"] = train[feature].replace(dic3).astype(float)\n            test_copy[feature+\"_count_label\"] = test[feature].replace(dic3).astype(float)\n            \n            temp_cols = [feature + \"_count\", feature + \"_count_label\"]\n            \n        else:\n            # For categorical features, use only count encoding\n            temp_cols = [feature + \"_count\"]\n        \n        # Handle high cardinality\n        if train_copy[feature].nunique() <= 25:\n            train_copy, test_copy = OHE(train_copy, test_copy, [feature], target)\n        else:\n            train_copy, test_copy = high_freq_ohe(train_copy, test_copy, [feature], target, n_limit=25)\n        \n        train_copy = train_copy.drop(columns=[feature])\n        test_copy = test_copy.drop(columns=[feature])\n        \n        # Cross validation\n        kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n        acc_scores = []\n        \n        for f in temp_cols:\n            if f not in train_copy.columns:\n                continue\n                \n            X = train_copy[[f]].values\n            y = train_copy[target].values\n            \n            cv_scores = []\n            for train_idx, val_idx in kf.split(X, y):\n                X_train, y_train = X[train_idx], y[train_idx]\n                x_val, y_val = X[val_idx], y[val_idx]\n                model = lgb.LGBMClassifier(random_state=RANDOM_STATE,verbose=-1)\n                \n                model.fit(X_train, y_train)\n                y_pred = model.predict(x_val)\n                cv_scores.append(accuracy_score(y_val, y_pred))\n                \n            acc_scores.append((f, np.mean(cv_scores)))\n            if overall_best_score < np.mean(cv_scores):\n                overall_best_score = np.mean(cv_scores)\n                overall_best_col = f\n        \n        if acc_scores:  # Only if we have scores to process\n            best_col, best_acc = sorted(acc_scores, key=lambda x: x[1], reverse=True)[0]\n            \n            # Feature selection based on correlation\n            available_cols = [col for col in temp_cols if col in train_copy.columns]\n            if len(available_cols) > 1:\n                corr = train_copy[available_cols].corr(method='pearson')\n                corr_with_best_col = corr[best_col]\n                cols_to_drop = [f for f in available_cols if corr_with_best_col[f] > 0.5 and f != best_col]\n                \n                # if cols_to_drop:\n                #     train_copy = train_copy.drop(columns=cols_to_drop)\n                #     test_copy = test_copy.drop(columns=cols_to_drop)\n                    \n            table.add_row([feature, best_col, best_acc])\n    \n    print(table)\n    print(\"Overall best CV score: \", overall_best_score)\n    return train_copy, test_copy\n\ntrain_processed, test_processed, cat_cols_updated = handle_categorical_features(train, test, categorical_features)\ntrain, test = cat_encoding(train_processed, test_processed, cat_cols_updated, 'Depression')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:18:59.350911Z","iopub.execute_input":"2024-11-18T14:18:59.351341Z","iopub.status.idle":"2024-11-18T14:19:27.318673Z","shell.execute_reply.started":"2024-11-18T14:18:59.3513Z","shell.execute_reply":"2024-11-18T14:19:27.317347Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.3 ARITHMETIC FEATURES","metadata":{}},{"cell_type":"code","source":"def better_features(train, test, target, cols, best_score):\n    new_cols = []\n    skf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)  # Stratified k-fold object\n    best_list=[]\n    for i in tqdm(range(len(cols)), desc='Generating Columns'):\n        col1 = cols[i]\n        temp_df = pd.DataFrame()  # Temporary dataframe to store the generated columns\n        temp_df_test = pd.DataFrame()  # Temporary dataframe for test data\n\n        for j in range(i+1, len(cols)):\n            col2 = cols[j]\n            # Multiply\n            temp_df[col1 + '*' + col2] = train[col1] * train[col2]\n            temp_df_test[col1 + '*' + col2] = test[col1] * test[col2]\n\n            # Divide (col1 / col2)\n            temp_df[col1 + '/' + col2] = train[col1] / (train[col2] + 1e-5)\n            temp_df_test[col1 + '/' + col2] = test[col1] / (test[col2] + 1e-5)\n\n            # Divide (col2 / col1)\n            temp_df[col2 + '/' + col1] = train[col2] / (train[col1] + 1e-5)\n            temp_df_test[col2 + '/' + col1] = test[col2] / (test[col1] + 1e-5)\n\n            # Subtract\n            temp_df[col1 + '-' + col2] = train[col1] - train[col2]\n            temp_df_test[col1 + '-' + col2] = test[col1] - test[col2]\n\n            # Add\n            temp_df[col1 + '+' + col2] = train[col1] + train[col2]\n            temp_df_test[col1 + '+' + col2] = test[col1] + test[col2]\n\n        SCORES = []\n        for column in temp_df.columns:\n            scores = []\n            for train_index, val_index in skf.split(train, train[target]):\n                X_train, X_val = temp_df[column].iloc[train_index].values.reshape(-1, 1), temp_df[column].iloc[val_index].values.reshape(-1, 1)\n                y_train, y_val = train[target].iloc[train_index], train[target].iloc[val_index]\n                model =lgb.LGBMClassifier(random_state=RANDOM_STATE,verbose=-1)\n                model.fit(X_train, y_train)\n                y_pred = model.predict(X_val)\n                score = accuracy_score(y_val,y_pred)\n                scores.append(score)\n            mean_score = np.mean(scores)\n            SCORES.append((column, mean_score))\n\n        if SCORES:\n            best_col, best_acc = sorted(SCORES, key=lambda x: x[1],reverse=True)[0]\n            corr_with_other_cols = train.drop([target] + new_cols, axis=1).corrwith(temp_df[best_col])\n            if (corr_with_other_cols.abs().max() < 0.9 or best_acc > best_score) and corr_with_other_cols.abs().max() !=1 :\n                train[best_col] = temp_df[best_col]\n                test[best_col] = temp_df_test[best_col]\n                new_cols.append(best_col)\n                print(f\"Added column '{best_col}' with Accuracy Score: {best_acc:.4f} & Correlation {corr_with_other_cols.abs().max():.4f}\")\n\n    return train, test, new_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:19:27.320702Z","iopub.execute_input":"2024-11-18T14:19:27.321128Z","iopub.status.idle":"2024-11-18T14:19:27.342457Z","shell.execute_reply.started":"2024-11-18T14:19:27.321086Z","shell.execute_reply":"2024-11-18T14:19:27.341184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_features=[f for f in test.columns if train[f].nunique()>2]\nlen(selected_features)\n\n# train, test,new_cols=better_features(train, test, target, selected_features, overall_best_score)\nnew_cols=['Work Pressure*Work/Study Hours',\n 'CGPA*Financial Stress',\n 'Job Satisfaction*Work/Study Hours',\n 'Work/Study Hours*Profession_count',\n 'Financial Stress*Profession_count',\n 'Min_sleep*Profession_count']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:19:27.344361Z","iopub.execute_input":"2024-11-18T14:19:27.344897Z","iopub.status.idle":"2024-11-18T14:19:27.542883Z","shell.execute_reply.started":"2024-11-18T14:19:27.34484Z","shell.execute_reply":"2024-11-18T14:19:27.54156Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<font size=\"3\">We don't have to run the above algorithm every time, we can just store the combinations and compute the required columns. Unimportant columns and binary columns are not considered while assessing the combinations</font>","metadata":{}},{"cell_type":"code","source":"def apply_arithmetic_operations(train_df, test_df, expressions_list):\n    for expression in expressions_list:\n        if expression not in train_df.columns:\n            # Split the expression based on operators (+, -, *, /)\n            parts = expression.split('+') if '+' in expression else \\\n                    expression.split('-') if '-' in expression else \\\n                    expression.split('*') if '*' in expression else \\\n                    expression.split('/')\n\n            # Get the DataFrame column names involved in the operation\n            cols = [col for col in parts]\n\n            # Perform the corresponding arithmetic operation based on the operator in the expression\n            if cols[0] in train_df.columns and cols[1] in train_df.columns:\n                if '+' in expression:\n                    train_df[expression] = train_df[cols[0]] + train_df[cols[1]]\n                    test_df[expression] = test_df[cols[0]] + test_df[cols[1]]\n                elif '-' in expression:\n                    train_df[expression] = train_df[cols[0]] - train_df[cols[1]]\n                    test_df[expression] = test_df[cols[0]] - test_df[cols[1]]\n                elif '*' in expression:\n                    train_df[expression] = train_df[cols[0]] * train_df[cols[1]]\n                    test_df[expression] = test_df[cols[0]] * test_df[cols[1]]\n                elif '/' in expression:\n                    train_df[expression] = train_df[cols[0]] / (train_df[cols[1]]+1e-5)\n                    test_df[expression] = test_df[cols[0]] /( test_df[cols[1]]+1e-5)\n    \n    return train_df, test_df\n\ntrain, test = apply_arithmetic_operations(train, test, new_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:19:27.545581Z","iopub.execute_input":"2024-11-18T14:19:27.546001Z","iopub.status.idle":"2024-11-18T14:19:27.574162Z","shell.execute_reply.started":"2024-11-18T14:19:27.545959Z","shell.execute_reply":"2024-11-18T14:19:27.572637Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6.1 FEATURE SELECTION","metadata":{}},{"cell_type":"code","source":"final_features=[f for f in train.columns if f not in [target] and train[f].dtype== float]\nfinal_features=[*set(final_features)]\n\nsc=StandardScaler()\n\ntrain_scaled=train.copy()\ntest_scaled=test.copy()\ntrain_scaled[final_features]=sc.fit_transform(train[final_features])\ntest_scaled[final_features]=sc.transform(test[final_features])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:19:27.575722Z","iopub.execute_input":"2024-11-18T14:19:27.576132Z","iopub.status.idle":"2024-11-18T14:19:28.156451Z","shell.execute_reply.started":"2024-11-18T14:19:27.576091Z","shell.execute_reply":"2024-11-18T14:19:28.155495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def post_processor(train, test, target):\n    cols = train.drop(columns=[target]).columns\n    train_cop = train.copy()\n    test_cop = test.copy()\n    drop_cols = []\n    \n    col_hash_map = {}\n\n    for feature in cols:\n        col_hash = pd.util.hash_pandas_object(train_cop[feature], index=False).sum()\n\n        if col_hash in col_hash_map:\n            drop_cols.append(feature)\n        else:\n            col_hash_map[col_hash] = feature\n\n    print(\"Columns to drop:\", drop_cols)\n    \n    train_cop.drop(columns=drop_cols, inplace=True)\n    test_cop.drop(columns=drop_cols, inplace=True)\n    \n    return train_cop, test_cop\n\ntrain_cop, test_cop = post_processor(train_scaled, test_scaled, target)\n# train_cop, test_cop = train_scaled.copy(), test_scaled.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:19:28.158292Z","iopub.execute_input":"2024-11-18T14:19:28.158783Z","iopub.status.idle":"2024-11-18T14:19:28.673056Z","shell.execute_reply.started":"2024-11-18T14:19:28.158729Z","shell.execute_reply":"2024-11-18T14:19:28.671679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = train_cop.drop(columns=[target])\ny_train = train[target]\n\nX_test = test_cop.copy()\n\ncat_features = X_train.select_dtypes(include=['object']).columns.tolist() \n\nX_train[cat_features]=X_train[cat_features].astype(str).fillna('None')\nX_test[cat_features]=X_test[cat_features].astype(str).fillna('None')\n\nprint(X_train.shape, X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:19:28.674654Z","iopub.execute_input":"2024-11-18T14:19:28.675149Z","iopub.status.idle":"2024-11-18T14:19:28.77528Z","shell.execute_reply.started":"2024-11-18T14:19:28.675094Z","shell.execute_reply":"2024-11-18T14:19:28.774206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef get_most_important_features(X_train, y_train, n, model_input, device='cpu', RANDOM_STATE=42):\n    \"\"\"\n    Get most important features using various boosting algorithms\n    \n    Parameters:\n    -----------\n    X_train : pd.DataFrame\n        Training features\n    y_train : pd.Series\n        Target variable\n    n : int\n        Number of top features to return\n    model_input : str\n        Type of model to use ('xgb', 'cat', or 'lgb')\n    device : str\n        Device to use for computation ('cpu' or 'gpu')\n    RANDOM_STATE : int\n        Random seed for reproducibility\n    \n    Returns:\n    --------\n    list\n        Top n most important features\n    \"\"\"\n    # Set style\n    plt.style.use('seaborn')\n    sns.set_palette(\"husl\")\n    \n    # Model parameters\n    xgb_params = {\n        'n_jobs': -1,\n        'tree_method': 'hist',\n        'verbosity': 0,\n        'random_state': RANDOM_STATE,\n    }\n    \n    lgb_params = {\n        'boosting_type': 'gbdt',\n        'random_state': RANDOM_STATE,\n        'device': device.lower(),\n        'verbose': -1,\n        'n_jobs': -1,\n        'metric': 'accuracy'\n    }\n    \n    cb_params = {\n        'task_type': device.upper(),\n        'random_state': RANDOM_STATE,\n        'verbose': False,\n        'thread_count': -1\n    }\n    \n    # GPU configurations\n    if device.lower() == 'gpu':\n        xgb_params.update({\n            'tree_method': 'gpu_hist',\n            'predictor': 'gpu_predictor'\n        })\n        \n    # Model selection\n    if 'xgb' in model_input.lower():\n        model = xgb.XGBClassifier(**xgb_params)\n    elif 'cat' in model_input.lower():\n        model = CatBoostClassifier(**cb_params)\n    else:\n        model = lgb.LGBMClassifier(**lgb_params)\n    \n    # Cross validation setup\n    kfold = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n    f1_scores = []\n    feature_importances_list = []\n    \n    # Perform cross-validation\n    for train_idx, val_idx in kfold.split(X_train):\n        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n        \n        # Model fitting\n        if 'lgb' in model_input.lower():\n            model.fit(X_train_fold, y_train_fold)\n        else:\n            model.fit(X_train_fold, y_train_fold, verbose=False)\n        \n        # Predictions and scoring\n        y_pred = model.predict(X_val_fold)\n        f1_scores.append(accuracy_score(y_val_fold, y_pred))\n        feature_importances_list.append(model.feature_importances_)\n    \n    # Calculate average metrics\n    avg_accuracy = np.mean(f1_scores)\n    avg_feature_importances = np.mean(feature_importances_list, axis=0)\n    \n    # Get feature importance rankings\n    feature_importance_list = list(zip(X_train.columns, avg_feature_importances))\n    sorted_features = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n    top_n_features = [feature[0] for feature in sorted_features[:n]]\n    display_features = top_n_features[:12]\n    \n    # Plotting\n    plt.figure(figsize=(12, 8))\n    \n    # Create color palette\n    colors = sns.color_palette(\"husl\", n_colors=1)\n    \n    # Plot horizontal bars\n    bars = plt.barh(range(len(display_features)), \n                   [avg_feature_importances[X_train.columns.get_loc(feature)] for feature in display_features],\n                   color=colors)\n    \n    # Customize plot\n    plt.yticks(range(len(display_features)), display_features, fontsize=10)\n    plt.xlabel('Feature Importance Score', fontsize=12)\n    plt.ylabel('Features', fontsize=12)\n    plt.title(f'Top {len(display_features)} Most Important Features\\nAverage Accuracy: {avg_accuracy:.4f}', \n              fontsize=14, pad=20)\n    \n    # Invert y-axis and add grid\n    plt.gca().invert_yaxis()\n    plt.grid(axis='x', linestyle='--', alpha=0.3)\n    \n    # Add value labels on bars\n    for i, bar in enumerate(bars):\n        width = bar.get_width()\n        plt.text(width + 0.001, bar.get_y() + bar.get_height()/2,\n                f'{width:.3f}',\n                ha='left', va='center', fontsize=10)\n    \n    # Adjust layout and display\n    plt.tight_layout()\n    plt.show()\n    \n    return top_n_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:19:44.408852Z","iopub.execute_input":"2024-11-18T14:19:44.409366Z","iopub.status.idle":"2024-11-18T14:19:44.434887Z","shell.execute_reply.started":"2024-11-18T14:19:44.409321Z","shell.execute_reply":"2024-11-18T14:19:44.433578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_imp_features_cat=get_most_important_features(X_train.reset_index(drop=True), y_train,100, 'cat', device=device)\nn_imp_features_xgb=get_most_important_features(X_train.reset_index(drop=True), y_train,100, 'xgb',  device=device)\nn_imp_features_lgbm=get_most_important_features(X_train.reset_index(drop=True), y_train,100, 'lgbm',  device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:19:44.696552Z","iopub.execute_input":"2024-11-18T14:19:44.696983Z","iopub.status.idle":"2024-11-18T14:19:44.702423Z","shell.execute_reply.started":"2024-11-18T14:19:44.696943Z","shell.execute_reply":"2024-11-18T14:19:44.701089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_imp_features=[*set(n_imp_features_xgb+n_imp_features_lgbm+n_imp_features_cat)]\nprint(f\"{len(n_imp_features)} features have been selected from three algorithms for the final model\")\n\nX_train=X_train[n_imp_features]\nX_test=X_test[n_imp_features]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:19:44.99058Z","iopub.execute_input":"2024-11-18T14:19:44.991703Z","iopub.status.idle":"2024-11-18T14:19:44.996313Z","shell.execute_reply.started":"2024-11-18T14:19:44.991654Z","shell.execute_reply":"2024-11-18T14:19:44.995015Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. MODEL","metadata":{}},{"cell_type":"markdown","source":"## 7.1 ANNs","metadata":{}},{"cell_type":"code","source":"import tensorflow\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LeakyReLU, PReLU, ELU\nfrom keras.layers import Dropout\n\nfrom keras.utils import to_categorical\n\ngpus = tensorflow.config.list_physical_devices('GPU')\nif gpus:\n    print(\"GPU is available\")\nelse:\n    print(\"GPU is not available\")\n    \n    \ndef optimizer():\n    sgd=tensorflow.keras.optimizers.SGD(learning_rate=0.005, momentum=0.5, nesterov=True)\n    rms = tensorflow.keras.optimizers.RMSprop()\n    nadam=tensorflow.keras.optimizers.Nadam(\n        learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n    )\n    adam=tensorflow.keras.optimizers.Adam()\n    adamW = keras.optimizers.AdamW(learning_rate=0.002,weight_decay=0.001, beta_1=0.9, beta_2=0.999)\n    \n    return sgd,rms,nadam, adamW,adam\n\n\nlrelu = lambda x: tensorflow.keras.activations.relu(x, alpha=0.1)\n\nfrom tensorflow.keras import backend as K\n\ndef root_mean_squared_log_error(y_true, y_pred):\n    '''\n    Compute RMSLE between actuals & predictions\n    '''\n    return K.sqrt(K.mean(K.square(K.log(abs(y_pred+1)) - K.log(abs(y_true+1)))))\n    \ndef root_mean_squared_error(y_true, y_pred):\n    '''\n    Compute RMSE between actuals & predictions\n    '''\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:19:45.982884Z","iopub.execute_input":"2024-11-18T14:19:45.983315Z","iopub.status.idle":"2024-11-18T14:19:46.001751Z","shell.execute_reply.started":"2024-11-18T14:19:45.983276Z","shell.execute_reply":"2024-11-18T14:19:46.000338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def init_ann1(num_classes, input_dim):\n    '''\n    Initialize the artificial neural network (ANN) for multiclass classification\n    '''\n\n    sgd, rms, nadam, adamW, adam = optimizer()\n    \n    ann = Sequential()\n    ann.add(Dense(16, input_dim=input_dim, kernel_initializer='he_uniform', activation='relu'))\n    ann.add(Dropout(0.1))\n    ann.add(Dense(8, kernel_initializer='he_uniform', activation='relu'))\n    ann.add(Dropout(0.1))\n    ann.add(Dense(4, kernel_initializer='he_uniform', activation='relu'))\n    ann.add(Dropout(0.0))\n    \n    # Change the output layer to match the number of classes\n    ann.add(Dense(num_classes, kernel_initializer='he_uniform', activation='softmax'))\n    \n    # Compile the model with categorical_crossentropy loss and accuracy metric\n    ann.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    \n    return ann\n\ndef init_ann2(num_classes, input_dim):  \n    sgd,rms,nadam, adamW, adam=optimizer()\n    ann2 = Sequential()\n    ann2.add(Dense(128, input_dim=X_test.shape[1], kernel_initializer='he_uniform', activation='relu'))\n    ann2.add(Dropout(0.3))\n    ann2.add(Dense(32,  kernel_initializer='he_uniform', activation='relu'))\n    ann2.add(Dropout(0.1))\n    ann2.add(Dense(4,  kernel_initializer='he_uniform', activation='relu'))\n    ann2.add(Dropout(0.2))\n#     ann2.add(Dense(16,  kernel_initializer='he_uniform', activation='relu'))\n#     ann2.add(Dropout(0.1))\n\n    ann2.add(Dense(num_classes, kernel_initializer='he_uniform', activation='softmax'))\n    ann2.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    \n    return ann2\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:19:46.490169Z","iopub.execute_input":"2024-11-18T14:19:46.490651Z","iopub.status.idle":"2024-11-18T14:19:46.504048Z","shell.execute_reply.started":"2024-11-18T14:19:46.490605Z","shell.execute_reply":"2024-11-18T14:19:46.502792Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.2 DEFINE MODELS","metadata":{}},{"cell_type":"code","source":"class Splitter:\n    def __init__(self, test_size=0.2, kfold=True, n_splits=5):\n        self.test_size = test_size\n        self.kfold = kfold\n        self.n_splits = n_splits\n\n    def split_data(self, X, y, random_state_list):\n        if self.kfold:\n            for random_state in random_state_list:\n                kf = StratifiedKFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n                for train_index, val_index in kf.split(X, y):\n                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n                    yield X_train, X_val, y_train, y_val\n                    \nclass Classifier:\n    def __init__(self, n_estimators=100, device=\"cpu\", random_state=0):\n        self.n_estimators = n_estimators\n        self.device = device\n        self.random_state = random_state\n        self.models = self._define_model()\n        self.len_models = len(self.models)\n    def _define_model(self):\n       \n       # XGBoost parameters\n       xgb_params = {\n           'n_estimators': self.n_estimators,\n           'learning_rate': 0.05,\n           'max_depth': 4,\n           'subsample': 0.8,\n           'colsample_bytree': 0.1,\n           'n_jobs': -1,\n           'eval_metric': 'auc',  # Changed to binary error\n           'objective': 'binary:logistic',  # Changed to binary objective\n           'tree_method': 'hist',\n           'verbosity': 0,\n           'random_state': self.random_state,\n       }\n       \n       if self.device == 'gpu':\n           xgb_params['tree_method'] = 'gpu_hist'\n           xgb_params['predictor'] = 'gpu_predictor'\n    \n       xgb_params2 = {\n           'n_estimators': self.n_estimators,\n           'gamma': 0.279,\n           'max_depth': 10,\n           'subsample': 0.325,\n           'min_child_weight': 9,\n           'colsample_bytree': 0.487,\n           'learning_rate': 0.052,\n           'reg_lambda': 0.0007,\n           'reg_alpha': 0.371,\n           'n_jobs': -1,\n           'eval_metric': 'auc',  # Changed to binary\n           'objective': 'binary:logistic',  # Changed to binary\n           'tree_method': 'hist',\n           'verbosity': 0,\n           'random_state': self.random_state,\n       }\n    \n       xgb_params3 = {\n           'n_estimators': self.n_estimators,\n           'gamma': 0.279,\n           'max_depth': 10,\n           'subsample': 0.325,\n           'min_child_weight': 9,\n           'colsample_bytree': 0.487,\n           'learning_rate': 0.052,\n           'reg_lambda': 0.0007,\n           'reg_alpha': 0.371,\n           'n_jobs': -1,\n           'eval_metric': 'auc',\n           'objective': 'binary:logistic',\n           'tree_method': 'hist',\n           'verbosity': 0,\n           'device': 'cuda',\n           'booster': 'gbtree',\n           'random_state': self.random_state,\n       }\n    \n       xgb_params4 = xgb_params.copy()\n       xgb_params4.update({\n           'subsample': 0.789,\n           'max_depth': 5,\n           'learning_rate': 0.161,\n           'colsample_bytree': 0.243\n       })\n    \n       xgb_params5 = xgb_params.copy()\n       xgb_params5['objective'] = \"binary:logistic\"\n    \n       # LightGBM parameters\n       lgb_params = {\n           'n_estimators': 1024,\n           'max_depth': 11,\n           'min_samples_leaf': 26,\n           'subsample': 0.912,\n           'learning_rate': 0.029,\n           'lambda_l1': 4.161,\n           'lambda_l2': 2.636e-05,\n           'colsample_bytree': 0.206,\n           'objective': 'binary',  # Changed to binary\n           'metric': 'auc',  # Changed to binary error\n           'boosting_type': 'gbdt',\n           'device': self.device,\n           'random_state': self.random_state,\n           'verbose': -1\n       }\n    \n       lgb_params2 = {\n           'n_estimators': 1000,\n           'max_depth': 6,\n           'subsample': 0.743,\n           'learning_rate': 0.049,\n           'lambda_l1': 8.922e-05,\n           'lambda_l2': 0.0018,\n           'colsample_bytree': 0.392,\n           'objective': 'binary',\n           'metric': 'auc',\n           'boosting_type': 'gbdt',\n           'device': self.device,\n           'random_state': self.random_state,\n           'verbose': -1\n       }\n    \n       lgb_params3 = {\n           'n_estimators': 1000,\n           'max_depth': 9,\n           'subsample': 0.540,\n           'learning_rate': 0.049,\n           'lambda_l1': 1.749e-08,\n           'lambda_l2': 3.837,\n           'colsample_bytree': 0.319,\n           'objective': 'binary',\n           'metric': 'auc',\n           'boosting_type': 'gbdt',\n           'device': self.device,\n           'random_state': self.random_state,\n           'verbose': -1\n       }\n    \n       lgb_params4 = lgb_params2.copy()\n       lgb_params4.update({\n           'subsample': 0.9,\n           'reg_lambda': 0.876,\n           'reg_alpha': 0.319,\n           'max_depth': 9,\n           'learning_rate': 0.107,\n           'colsample_bytree': 0.1\n       })\n    \n       lgb_params5 = lgb_params2.copy()\n       lgb_params5.update({\n           'subsample': 0.9,\n           'reg_lambda': 0.512,\n           'reg_alpha': 0.898,\n           'max_depth': 11,\n           'learning_rate': 0.081,\n           'colsample_bytree': 0.1\n       })\n    \n       # CatBoost parameters\n       cb_params = {\n           'iterations': self.n_estimators,\n           'depth': 6,\n           'learning_rate': 0.05,\n           'l2_leaf_reg': 0.7,\n           'random_strength': 0.2,\n           'max_bin': 200,\n           'od_wait': 65,\n           'one_hot_max_size': 70,\n           'grow_policy': 'Depthwise',\n           'bootstrap_type': 'Bayesian',\n           'od_type': 'Iter',\n           'eval_metric': 'AUC',\n           'loss_function': 'Logloss',  # Changed to binary\n           'task_type': self.device.upper(),\n           'random_state': self.random_state,\n           'verbose': -1\n       }\n    \n       cb_sym_params = cb_params.copy()\n       cb_sym_params['grow_policy'] = 'SymmetricTree'\n    \n       cb_loss_params = cb_params.copy()\n       cb_loss_params['grow_policy'] = 'Lossguide'\n    \n       cb_params2 = cb_params.copy()\n       cb_params2.update({\n           'learning_rate': 0.019,\n           'depth': 9,\n           'random_strength': 0.3,\n           'one_hot_max_size': 10,\n           'max_bin': 100,\n           'l2_leaf_reg': 0.419\n       })\n    \n       cb_params3 = {\n           'iterations': self.n_estimators,\n           'random_strength': 0.1,\n           'one_hot_max_size': 70,\n           'learning_rate': 0.008,\n           'l2_leaf_reg': 0.3,\n           'grow_policy': 'Depthwise',\n           'depth': 9,\n           # 'max_bin': 200,\n           'od_wait': 65,\n           'bootstrap_type': 'Bayesian',\n           'od_type': 'Iter',\n           'eval_metric': 'AUC',\n           'loss_function': 'Logloss',\n           'task_type': self.device.upper(),\n           'random_state': self.random_state,\n       }\n    \n       cb_params4 = cb_params.copy()\n       cb_params4.update({\n           'learning_rate': 0.143,\n           'depth': 16,\n           'random_strength': 0.596,\n           # 'one_hot_max_size': 100,\n           # 'max_bin': 150,\n           'l2_leaf_reg': 0.384,\n           'grow_policy': 'Lossguide'\n       })\n    \n       # Other model parameters remain same\n       dt_params = {'criterion': 'gini', 'max_depth': 9, 'min_samples_split': 17, 'min_samples_leaf': 18, 'max_features': 0.843}\n       etr_params = {'criterion': 'gini', 'max_depth': 16, 'min_samples_split': 11, 'min_samples_leaf': 1, 'max_features': 0.668, 'bootstrap': True}\n       hist_params = {'learning_rate': 0.058, 'n_iter_no_change': 795, 'max_depth': 4, 'min_samples_leaf': 17, 'max_leaf_nodes': 98, 'l2_regularization': 1.923e-07}\n       rf_params = {'max_depth': 16, 'min_samples_split': 18, 'min_samples_leaf': 2, 'max_features': 0.416}\n       gbt_params = {'learning_rate': 0.136, 'max_depth': 7, 'min_samples_split': 17, 'min_samples_leaf': 15, 'subsample': 0.886, 'max_features': 0.611}\n       knn_params = {'n_neighbors': 16, 'weights': 'uniform', 'p': 2, 'leaf_size': 13, 'algorithm': 'ball_tree'}\n       adb_params = {'n_estimators': 957, 'learning_rate': 0.663}\n    \n       models = {\n           'xgb':  xgb.XGBClassifier(**xgb_params),\n           'xgb2': xgb.XGBClassifier(**xgb_params2),\n           'xgb3': xgb.XGBClassifier(**xgb_params3),\n           'xgb4': xgb.XGBClassifier(**xgb_params4),\n           'xgb5': xgb.XGBClassifier(**xgb_params5),\n           'lgb':  lgb.LGBMClassifier(**lgb_params),\n           'lgb2': lgb.LGBMClassifier(**lgb_params2),\n           'lgb3': lgb.LGBMClassifier(**lgb_params3),\n           'lgb4': lgb.LGBMClassifier(**lgb_params4),\n           'lgb5': lgb.LGBMClassifier(**lgb_params5),\n           'cat':  CatBoostClassifier(**cb_params),\n           'cat2': CatBoostClassifier(**cb_params2),\n           'cat3': CatBoostClassifier(**cb_params3),\n           'cat4': CatBoostClassifier(**cb_params4),\n           \"cat_sym\": CatBoostClassifier(**cb_sym_params),\n           \"cat_loss\": CatBoostClassifier(**cb_loss_params),\n           'hist_gbm': HistGradientBoostingClassifier(max_iter=self.n_estimators, **hist_params, random_state=self.random_state),\n           'rf': RandomForestClassifier(n_estimators=250, **rf_params, random_state=self.random_state),\n           'gbdt': GradientBoostingClassifier(**gbt_params, n_estimators=100, random_state=self.random_state),            \n           # 'ada': AdaBoostClassifier(**adb_params, random_state=self.random_state),\n           'etr': ExtraTreesClassifier(**etr_params, random_state=self.random_state),\n           # 'dt': DecisionTreeClassifier(**dt_params, random_state=self.random_state),\n           # 'knn': KNeighborsClassifier(**knn_params),\n           # 'log_reg': LogisticRegression(max_iter=1000),\n           # 'ridge': CalibratedClassifierCV(RidgeClassifierCV(alphas=[100.02]), method='sigmoid'),\n           # 'elasticNet': LogisticRegressionCV(Cs=[0.044], l1_ratios=[0.977]),\n           # 'ann1': init_ann1(1, X_test.shape[1]),  # Changed to 1 for binary\n           # 'ann2': init_ann2(1, X_test.shape[1]),  # Changed to 1 for binary\n       }\n       \n       return models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:20:42.459945Z","iopub.execute_input":"2024-11-18T14:20:42.460368Z","iopub.status.idle":"2024-11-18T14:20:42.505514Z","shell.execute_reply.started":"2024-11-18T14:20:42.460329Z","shell.execute_reply":"2024-11-18T14:20:42.504216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.3 WEIGHTED ENSEMBLE","metadata":{}},{"cell_type":"code","source":"class OptunaWeights:\n    def __init__(self, random_state, n_trials=5000):\n        self.study = None\n        self.weights = None\n        self.random_state = random_state\n        self.n_trials = n_trials\n\n    def _objective(self, trial, y_true, y_preds):\n        # Define the weights for the predictions from each model\n        weights = [trial.suggest_float(f\"weight{n}\", 0, 1) for n in range(len(y_preds))]\n\n        # Calculate the weighted prediction\n        weighted_pred = np.average(np.array(y_preds), axis=0, weights=weights)\n        weighted_pred = weighted_pred/weighted_pred.sum(axis=1, keepdims=True)\n\n        threshold = find_best_threshold(y_true, weighted_pred)\n        weighted_pred_labels = (weighted_pred > threshold).astype(int)\n        accuracy = accuracy_score(y_true, weighted_pred_labels)\n        auc_score= roc_auc_score(y_true, weighted_pred)\n        return accuracy*(auc_score)\n\n    def fit(self, y_true, y_preds):\n        optuna.logging.set_verbosity(optuna.logging.ERROR)\n        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n        pruner = optuna.pruners.HyperbandPruner()\n        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='maximize')\n        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n        self.study.optimize(objective_partial, n_trials=self.n_trials)\n        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n\n    def predict(self, y_preds):\n        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n        weighted_pred = np.average(np.array(y_preds), axis=0, weights=self.weights)\n        return weighted_pred\n\n    def fit_predict(self, y_true, y_preds):\n        self.fit(y_true, y_preds)\n        return self.predict(y_preds)\n    \n    def weights(self):\n        return self.weights\n\ndef find_best_threshold(y_true, y_pred_probabilities):\n   \"\"\"\n   Find optimal threshold for binary classification using accuracy\n   \"\"\"\n   def objective(threshold):\n       y_pred = (y_pred_probabilities >= threshold).astype(int)\n       return -accuracy_score(y_true, y_pred)  # Negative since we want to maximize\n   \n   result = minimize_scalar(objective, bounds=(0, 1), method='bounded')\n   return result.x\n\n\ndef compute_means_dict(X_train, missing_cols=None):\n    \"\"\"\n    Compute means dictionary from training data\n    \n    Parameters:\n    X_train (pandas.DataFrame): Training data\n    missing_cols (list): List of columns to compute means for. \n                        If None, will find columns with NaN values\n    \n    Returns:\n    dict: Dictionary mapping column names to their means\n    \"\"\"\n    # If missing_cols not provided, find columns with NaN values\n    if missing_cols is None:\n        missing_cols = [f for f in X_train.columns if X_train[f].isna().sum() > 0]\n    \n    # Compute means dictionary\n    means_dict = {}\n    for col in missing_cols:\n        means_dict[col] = X_train[col].mean()\n    \n    return means_dict\ndef fill_missing_with_means(df, means_dict):\n    \"\"\"\n    Fill missing values using pre-computed means dictionary\n    \n    Parameters:\n    df (pandas.DataFrame): Input data (can be train or test)\n    means_dict (dict): Dictionary of column means computed from training data\n    \n    Returns:\n    pandas.DataFrame: DataFrame with missing values filled\n    \"\"\"\n    # Create a copy to avoid modifying original data\n    df_filled = df.copy()\n    \n    # Fill missing values using the means dictionary\n    for col, mean_val in means_dict.items():\n        if col in df_filled.columns:  # Check if column exists in the dataframe\n            df_filled[col].fillna(mean_val, inplace=True)\n    \n    return df_filled\n\nmeans_dict=compute_means_dict(X_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T14:20:43.51583Z","iopub.execute_input":"2024-11-18T14:20:43.516971Z","iopub.status.idle":"2024-11-18T14:20:43.591018Z","shell.execute_reply.started":"2024-11-18T14:20:43.516923Z","shell.execute_reply":"2024-11-18T14:20:43.589792Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.4 FIT MODELS","metadata":{}},{"cell_type":"code","source":"kfold = True\nn_splits = 1 if not kfold else 5\nrandom_state = 42\nrandom_state_list = [RANDOM_STATE] \nn_estimators = 1000 \nearly_stopping_rounds = 300\nverbose = False\n\nsplitter = Splitter(kfold=kfold, n_splits=n_splits)\noof_predss = pd.DataFrame(np.zeros((X_train.shape[0], 1)))\ntest_predss = np.zeros((X_test.shape[0], 1))\nensemble_score = []\nensemble_acc_score = []\nweights = []\ntrained_models = {'xgb':[]}\nbest_thresholds = []  # Store best thresholds for each fold\n   \nfor i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n   n = i % n_splits\n   m = i // n_splits\n           \n   classifier = Classifier(n_estimators, device, random_state)\n   models = classifier.models\n   \n   oof_preds = []\n   test_preds = []\n   start_time_fold = time.time()\n   \n   # Train and predict with each model\n   for name, model in models.items():\n       start_time = time.time()\n       \n       # Model fitting\n       if ('xgb' in name) or ('lgb' in name) or ('cat' in name):\n           if 'lgb' in name:\n               model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)])\n           elif 'cat' in name:\n               model.fit(X_train_, y_train_, \n                        eval_set=[(X_val, y_val)],\n                        early_stopping_rounds=early_stopping_rounds,cat_features=cat_features,\n                        verbose=verbose)  \n           else:\n               model.fit(X_train_, y_train_, \n                        eval_set=[(X_val, y_val)],\n                        early_stopping_rounds=early_stopping_rounds,\n                        verbose=verbose)\n       elif 'ann' in name:\n           model.fit(fill_missing_with_means(X_train_, means_dict), y_train_,\n                    validation_data=(fill_missing_with_means(X_val, means_dict), y_val),\n                    batch_size=16,\n                    epochs=10,\n                    verbose=verbose)\n       else:\n           model.fit(fill_missing_with_means(X_train_, means_dict), y_train_)\n           \n       if name in trained_models.keys():\n           trained_models[f'{name}'].append(deepcopy(model))\n       \n       # Make predictions\n       if 'ann' in name:\n           test_pred = model.predict(fill_missing_with_means(X_test, means_dict)).reshape(-1, 1)\n           y_val_pred = model.predict(fill_missing_with_means(X_val, means_dict)).reshape(-1, 1)\n       elif ('xgb' in name) or ('lgb' in name) or ('cat' in name):\n           test_pred = model.predict_proba(X_test)[:, 1].reshape(-1, 1)\n           y_val_pred = model.predict_proba(X_val)[:, 1].reshape(-1, 1)\n       else:\n           test_pred = model.predict_proba(fill_missing_with_means(X_test, means_dict))[:, 1].reshape(-1, 1)\n           y_val_pred = model.predict_proba(fill_missing_with_means(X_val, means_dict))[:, 1].reshape(-1, 1)\n       \n       end_time = time.time()\n       time_taken = end_time - start_time\n       \n       # Find optimal threshold and calculate accuracy\n       # threshold = find_best_threshold(y_val, y_val_pred)\n       y_val_pred_labels = (y_val_pred > 0.5).astype(int)\n       accuracy = accuracy_score(y_val, y_val_pred_labels)\n       \n       print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] '\n             f'Accuracy Score: {accuracy:.5f}'\n             f'time taken: {time_taken:.3f} secs')\n       \n       oof_preds.append(y_val_pred)\n       test_preds.append(test_pred)\n   \n   # Optimize ensemble weights\n   optweights = OptunaWeights(random_state=random_state)\n   y_val_pred = optweights.fit_predict(y_val, oof_preds)\n   oof_predss.loc[X_val.index] = np.array(y_val_pred).reshape(-1, 1)\n   # Find optimal threshold for ensemble predictions\n   ensemble_threshold = find_best_threshold(y_val, y_val_pred)\n   best_thresholds.append(ensemble_threshold)\n   \n   # Calculate metrics using optimal threshold\n   y_val_pred_labels = (y_val_pred > ensemble_threshold).astype(int)\n   accuracy = accuracy_score(y_val, y_val_pred_labels)\n   \n   end_time_fold = time.time()\n   time_taken = end_time_fold - start_time_fold\n   \n   print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] '\n         f'-------------------> Accuracy Score: {accuracy:.5f} '\n         f'(threshold: {ensemble_threshold:.3f}), '\n         f'fold time taken: {time_taken:.5f} secs')\n   \n   ensemble_acc_score.append(accuracy)\n   weights.append(optweights.weights)\n   \n   # Predict test data using ensemble weights\n   test_preds = optweights.predict(test_preds)\n   test_predss += test_preds / (n_splits * len(random_state_list))\n   \n   gc.collect()\n\n# Calculate final predictions using average of best thresholds\nfinal_threshold = np.mean(best_thresholds)\nprint(f\"\\nFinal threshold: {final_threshold:.3f}\")\nfinal_predictions = (test_predss > final_threshold).astype(int).reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T14:23:17.201317Z","iopub.execute_input":"2024-11-18T14:23:17.201799Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.5 MODEL WEIGHTS","metadata":{}},{"cell_type":"code","source":"print(f\"\\nEnsemble CV Accuracy: {np.mean(ensemble_acc_score):.5f} ¬± {np.std(ensemble_acc_score):.5f}\")\n\n# Print the mean and standard deviation of the ensemble weights for each model\nprint('--- Model Weights ---')\nmean_weights = np.mean(weights, axis=0)\nstd_weights = np.std(weights, axis=0)\nfor name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n    print(f'{name}: {mean_weight:.5f} ¬± {std_weight:.5f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.6 SUBMISSION","metadata":{}},{"cell_type":"code","source":"oof_predss.columns=['final_threshold']\noof_predss.to_csv('oof_predss_v22.csv',index=False)\n\nsubmission = pd.read_csv('/kaggle/input/playground-series-s4e11/sample_submission.csv')\nsubmission[target] =  test_predss\nsubmission.to_csv('submission_pred_v22.csv',index=False)\n\nsubmission[target] =  final_predictions\nsubmission.to_csv('submission.csv',index=False)\n\n\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-10T03:12:21.820329Z","iopub.status.idle":"2024-11-10T03:12:21.820787Z","shell.execute_reply.started":"2024-11-10T03:12:21.820555Z","shell.execute_reply":"2024-11-10T03:12:21.820576Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7.7 MODE ","metadata":{}},{"cell_type":"markdown","source":"<font size=\"3\">Use external results under the assumption that mode on selective results may result in a better score</font>\n\nThanks to the below authors for their work:\n1) By [sunilkumarmuduli](https://www.kaggle.com/code/sunilkumarmuduli/rank-3-sol-sql-meets-machine-learning)\n2) By [jiaoyouzhang](https://www.kaggle.com/code/jiaoyouzhang/mental-health-ensemble-0-94397)\n3) By [chinmayadatt](https://www.kaggle.com/code/chinmayadatt/0-94434-ensemble-exploring-mental-health-data)\n4) By [swandipsingha](https://www.kaggle.com/code/swandipsingha/mental-health-ps4e11)","metadata":{}},{"cell_type":"code","source":"sub_ext1=pd.read_csv(\"/kaggle/input/rank-3-sol-sql-meets-machine-learning/submission.csv\")\nsub_ext2=pd.read_csv(\"/kaggle/input/mental-health-ensemble-0-94397/submission.csv\")\nsub_ext3=pd.read_csv(\"/kaggle/input/0-94434-ensemble-exploring-mental-health-data/submission.csv\")\nsub_ext4=pd.read_csv(\"/kaggle/input/mental-health-ps4e11/submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T16:44:28.300517Z","iopub.execute_input":"2024-11-18T16:44:28.301742Z","iopub.status.idle":"2024-11-18T16:44:28.349839Z","shell.execute_reply.started":"2024-11-18T16:44:28.301695Z","shell.execute_reply":"2024-11-18T16:44:28.348813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef create_mode_submission(submissions_list, submission_template, target_column=target):\n    \"\"\"\n    Create a new submission file with mode values from multiple submissions.\n    \n    Parameters:\n        submissions_list (list): List of DataFrames containing predictions\n        submission_template (pd.DataFrame): Template submission DataFrame to use for structure\n        target_column (str): Name of the target column to calculate mode for\n        \n    Returns:\n        pd.DataFrame: New submission DataFrame with mode predictions\n    \"\"\"\n    # Create a copy of the template submission\n    final_submission = submission_template.copy()\n    \n    # Stack all predictions into a single DataFrame\n    all_predictions = pd.concat([df[target_column] for df in submissions_list], axis=1)\n    \n    # Calculate mode for each row\n    # If there's no mode (all values different), take the first value\n    mode_predictions = all_predictions.mode(axis=1)\n    \n    # Handle cases where there might be multiple modes\n    final_predictions = mode_predictions.iloc[:, 0]  # Take first mode if multiple exist\n    \n    # Update the target column in the final submission\n    final_submission[target_column] = final_predictions\n    \n    return final_submission\n\n\nsubmissions = [sub_ext1, sub_ext2,sub_ext3, sub_ext4, submission]\nmode_submission = create_mode_submission(submissions, sub_ext1)\n\nmode_submission.to_csv('submission_mode.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T16:45:05.201431Z","iopub.execute_input":"2024-11-18T16:45:05.20183Z","iopub.status.idle":"2024-11-18T16:45:06.389586Z","shell.execute_reply.started":"2024-11-18T16:45:05.201771Z","shell.execute_reply":"2024-11-18T16:45:06.387705Z"}},"outputs":[],"execution_count":null}]}